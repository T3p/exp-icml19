We thank the reviewers for their suggestions.

R1
-Question 1: the justification of Assumption 1 is missing from the supplementary material by mistake. We summarize it here. First, note that the results are for a compact arm set, hence the maximum distance among policy hyperparameters is bounded. Depending on the parametrization (the policy design phase mentioned in the paper), one must ensure that the Renyi divergence is also bounded. As an example, for Gaussian hyperpolicies with fixed standard deviation there is no such problem, as the Renyi divergence (cf Gil et al.) is a continuous function of the mean parameter (a continuous function on a compact set is bounded). If the standard deviation (or covariance matrix) is also part of the parameter set, additional care must be placed on the allowed range. For \epsilon=1, the std of the target distribution must not be larger than twice the one of the behavioral for the divergence to be finite. Hence, it is enough to constrain the search within [\sigma_0, 2*\sigma_0]. We also suggest to initialize the hyperpolicy with 2*\sigma_0, since the algorithm will tend to focus on smaller stds (i.e., more deterministic behavior) as the learning proceeds. Similar measures can be taken for other kinds of hyperpolicies
-Question 2: a major concern expressed by R1 is the applicability of the results to hyper-policies other than Gaussians. We focused on those as they are by far the most used in PGPE-like algorithms. However, all our results generalize to other distributions. We do not see the computation of the Renyi divergence as an issue. "RÃ©nyi divergence measures for commonly used univariate continuous distributions." by Gil et al., 2013, reports explicit formulas for the Renyi of several continuous distributions, including some multivariate ones. Also, our assumptions are totally general, although ensuring them in practice could require some distribution-dependent measures. In principle, one could also use NN to parametrize the action policy, and then place a simple (e.g. Gaussian) hyperpolicy on top of it. The actual issue would be the high number of hyper-parameters, not the functional class of the approximator
-We focused on discretization in this work, but global optimization techniques could be employed as suggested by Srinivas et al., 2010. This would be future-work material
-We think diminishing our analysis as a "theoretical exercise" is unfair. Although we combine existing techniques, the problem setting is peculiar because of the special correlation among arms. In fact, although this may not be completely clear from the paper (however, cf the last paragraph of Section 4), it was studying the problem of exploration in policy search that we identified this kind of correlation, and built the MAB framework as a way to explicitly exploit it. As pointed out by R4, ours is the first work to apply OFU to PO while exploiting this shared information
-We agree that the paragraph starting from line 146 (left) could give the wrong impression that the variance of balance heuristic is the same as the one of mixture sampling. The true relation is made explicit in the proof of Lemma 1. We will clarify this in the main paper too
-Related work: adding more statistical sources would be definitely good. The ones suggested by R3 seem a good starting point
-Experiments: we agree including PGPE in the regret plot would be useful. Plotting the regret for the MC experiment would require to know the performance of the optimal policy. Studying why PGPE gives better results (despite not having any guarantees) is an interesting future direction. However, being this a first work on regret minimization in PO, we think the focus should be more on theory than on practical performance

R2
2.This kind of regret was achieved in the MAB literature, but is new for the policy optimization setting
3.This is why we focus on discretization approaches, but we plan to use global optimization methods in future work
4.Do you have any specific suggestions on how to improve the experimental section?
5.We will add more trials in the final version

R3
-What is not clear in Section 2.1, and how would you improve it?
-Thank you for suggesting additional related works, we will include them in the final version. The one by Elvira et al. also seems of practical relevance for our work

R4
Thank you for pointing out typos.
