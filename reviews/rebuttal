We thank the reviewers for their insightful suggestions and comments.

R1
-A major concern expressed by R1 is the applicability of the results to non-Gaussians hyperpolicies. We focused on those as they are, by far, the mostly used in parameter-based algorithms. However, all our results (including the discretization procedure) are independent of the hyperpolicy/policy model. We do not see the computation of the Renyi divergence as an issue: [1] reports explicit formulas for the Renyi of several continuous distributions, including some multivariate ones. Moreover, Assumption 1 is general, although ensuring it in practice requires distribution-dependent constraints (see next point). In principle, we can use NN to parametrize the policy, and then place a hyperpolicy (e.g. Gaussian) on top of it. Being a parameter-based approach, the actual issue would be the high number of hyper-parameters, not the functional class of the policy (NN).
-The justification of Assumption 1 is missing from the supplementary material by mistake. We summarize it here. First, note that the results are provided for a compact arm set, hence the maximum distance among the (hyper-)parameters is bounded. Additionally, we must ensure that the Renyi divergence is also bounded. When considering the divergence between a target distribution and a mixture of proposals it is enought that the divergence is finite between the target and one of the components of the mixture (this is visible in Appendix A), hence we will focus on constraints between behavioral/target pairs. As an example, for Gaussian distributions (policies or hyperpolicies) with fixed standard deviation (std) there is no such problem, as the Renyi divergence [1] is a continuous function of the mean parameter (a continuous function on a compact set is bounded). If the std (or covariance matrix) is also part of the parameter set, additional constraints are needed. For \epsilon=1, the std of the target distribution must not be larger than twice that of the behavioral for the divergence to be finite. Hence, given a minimum std \sigma_0 > 0, it is enough to constrain the search within [\sigma_0, 2*\sigma_0]. We also suggest to initialize the first behavioral distribution with 2*\sigma_0, so that the algorithm will focus on smaller stds (i.e., less stochastic behavior) as the learning proceeds. Similar constraints can be defined for other kinds of hyperpolicies [1]. We will add this appendix in the final version of the paper, discussing also the constraints needed for \epsilon \in (0,1]. 
-We focused on discretization in this work, but global optimization techniques could be employed as suggested by [2].
-We think diminishing our analysis as a "theoretical exercise" is unfair. Although the regret analysis is rather standard, the problem setting is peculiar because of the special correlation among arms. In fact, although this may not be completely clear from the paper (however, cf the last paragraph of Sec. 4), studying the problem of exploration in policy search we identified this kind of correlation, and built the MAB framework as a way to explicitly exploit it via MIS. Thus, OPTIMIST should not be seen an application of a MAB framework to policy search, rather as a way to formulate exploration in policy search as a MAB-like problem, exploiting its peculiar "structure". As pointed out by R4, ours is the first work that applies OFU to policy search while exploiting this shared information.
-We agree that the paragraph starting from line 146 (left) could give the wrong impression that the variance of the balance heuristic is the same as the one of mixture sampling. We are aware that this is not the case. However, in order to have a handy expression we resort to the bound in Lemma 1 in which the two cases coincide. We will clarify this in the main paper.
-Related work: adding more statistical sources would be definitely good. The ones suggested by R3 seem a good starting point.
-Experiments: including PGPE in the regret plot would be useful. However, this would require to know the performance of the optimal policy, which can only be approximated (differently for the LQG case in which we have a closed-form expression). Studying why PGPE gives better results (despite not having any guarantees) is an interesting future direction. However, being this a first work on regret minimization in policy search, we mainly focused on theory rather than practical performance.

R2
2.This kind of regret was achieved in the MAB literature, but it is new for the policy optimization setting.
3.This is why we focus on discretization approaches, but we plan to use global optimization methods [2] in future work.
4.Being the first work on optimistic exploration in policy search we preferred to focus on theory, rather than on extensive experimental evaluation.
5.We will add more trials in the final version.

R3
-We will revise Section 2.1, for the final version, to make it clearer.
-We thank the reviewer for suggesting additional related works, we will include them in the final version. Elvira et al. is of practical relevance for our work, as scalability when the number of behavioral distribution grows is an important issue.

R4
We appreciate that the reviewer captured the spirit of the paper and rewarded its novelty. We will fix the reported typos in the final version.

[1] Gil, Manuel, Fady Alajaji, and Tamas Linder. "RÃ©nyi divergence measures for commonly used univariate continuous distributions." Information Sciences 249 (2013): 124-131.
[2] Srinivas, Niranjan, et al. "Gaussian process optimization in the bandit setting: No regret and experimental design." arXiv preprint arXiv:0912.3995 (2009).
