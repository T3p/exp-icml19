We thank the reviewers for their suggestions.

R1
-We focused on Gaussian hyperpolicies as they are, by far, the most used in parameter-based algorithms. However, all our results (including the discretization) are independent of the hyperpolicy/policy model. We do not see the computation of the Renyi divergence as an issue: [1] reports formulas for the Renyi of several continuous distributions, including some multivariate ones. Assumption 1 is general, although ensuring it in practice requires distribution-dependent constraints (see next point). In principle, we can use NN to parametrize the policy, and then place a hyperpolicy (e.g. Gaussian) on top of it. Being a parameter-based approach, the actual issue would be the high number of hyper-parameters, not the functional class of the policy (NN).
-The justification of Assumption 1 is missing from the supplementary material by mistake. We summarize it here. First, note that the results are provided for a compact arm set, hence the maximum distance among the (hyper-)parameters is bounded. Additionally, we must ensure that the Renyi divergence is bounded. When considering the divergence between a target distribution and a mixture of behavioral distributions it is enough that the divergence is finite between the target and one of the components of the mixture (see Appendix A), hence we will focus on constraints between behavioral/target pairs. As an example, for Gaussian distributions (policies or hyperpolicies) with fixed standard deviation (std) there is no such problem, as the Renyi divergence [1] is a continuous function of the mean parameter (a continuous function on a compact set is bounded). If the std (or covariance matrix) is also part of the parameter set, additional constraints are needed. For \epsilon=1, the std of the target distribution must not be larger than twice that of the behavioral for the divergence to be finite. Hence, given a minimum std \sigma_0 > 0, it is enough to constrain the search within [\sigma_0, 2*\sigma_0]. We also suggest initializing the first behavioral distribution with 2*\sigma_0, so that the algorithm will move towards smaller stds (less stochastic behavior). Similar constraints can be defined for other kinds of hyperpolicies [1]. We will add the appendix in the final version of the paper, discussing also the constraints needed for \epsilon \in (0,1]. 
-We focused on discretization in this work, but global optimization techniques (Srinivas, et al.) can be explored as future work.
-We think diminishing our analysis as a "theoretical exercise" is unfair. Although the regret analysis is rather standard, the problem setting is peculiar because of the correlation among arms. Although this may not be clear from the paper (however, cf the last paragraph of Sec. 4), studying the problem of exploration in policy search we identified this kind of correlation. Thus, OPTIMIST should not be seen as just an application of a MAB framework to policy search, rather as a way to formulate exploration in policy search as a MAB-like problem, exploiting its peculiar structure. As pointed out by R4, ours is the first work that applies OFU to policy search while exploiting this structure.
-We agree that the paragraph starting from line 146 (left) could give the wrong impression that the variance of the balance heuristic is the same as the one of mixture sampling. We are aware that this is not the case. The true relation is made clear in the proof of Lemma 1. We will clarify this in the paper.
-Related work: we will add the references suggested by R3.
-Experiments: including PGPE in the regret plot would be useful. However, this requires to know the performance of the optimal policy, which can only be approximated (while for the LQG we have a closed-form). Studying why PGPE gives better results (despite not having any guarantees) is an interesting future direction. However, being this a first work on regret minimization in policy search, we mainly focused on theory rather than practical performance.

R2
2.This kind of regret was achieved in the MAB literature, but it is new for the policy optimization setting.
3.This is why we focus on discretization approaches, but we plan to use global optimization methods (Srinivas, et al.) in future works.
4.Being the first work on regret minimization in policy search we preferred to focus on theory, rather than on extensive experimental evaluation.
5.We will add more trials in the final version.

R3
-We will revise Section 2.1 to make it clearer.
-We thank the reviewer for suggesting additional related works, we will include them in the final version. Elvira et al. is of practical relevance for our work.

R4
We appreciate that the reviewer captured the spirit of the paper and rewarded its novelty. We will fix the reported typos.

[1] Gil, Manuel, Fady Alajaji, and Tamas Linder. "RÃ©nyi divergence measures for commonly used univariate continuous distributions." Information Sciences 249 (2013): 124-131.
