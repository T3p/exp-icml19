We thank the reviewers for their suggestions.

R1
-Question 1: the justification of Assumption 1 is missing from the supplementary material by mistake. We summarize it here. First, note that the results are for a compact arm set, hence the maximum distance among policy hyperparameters is bounded. Depending on the parametrization (the policy design phase mentioned in the paper), one must ensure that the Renyi divergence is also bounded. As an example, for Gaussian hyperpolicies with fixed standard deviation there is no such problem, as the Renyi divergence [1] is a continuous function of the mean parameter (a continuous function on a compact set is bounded). If the standard deviation (or covariance matrix) is also part of the parameter set, additional care must be placed on the allowed range. For \epsilon=1, the std of the target distribution must not be larger than twice the one of the behavioral for the divergence to be finite. When considering multiple behavioral distribution, it sufficies to have one of them fullfilling the requirement. Hence, given a minimum std \sigma_0 > 0, it is enough to constrain the search within [\sigma_0, 2*\sigma_0]. We also suggest to initialize the first behavioral distribution with 2*\sigma_0, since the algorithm will tend to focus on smaller stds (i.e., less stochastic behavior) as the learning proceeds. Similar constraints can be defined for other kinds of hyperpolicies (see Question 2).
-Question 2: a major concern expressed by R1 is the applicability of the results to hyperpolicies other than Gaussians. We focused on those as they are, by far, the most used in parameter-based algorithms. However, all our results (including the discretization procedure) are independent on the hyperpolicy model. We do not see the computation of the Renyi divergence as an issue: [1] reports explicit formulas for the Renyi of several continuous distributions, including some multivariate ones. Also, our assumptions are general, although ensuring them in practice could require some distribution-dependent constraints. In principle, one could also use NN to parametrize the action policy, and then place a simple (e.g. Gaussian) hyperpolicy on top of it. Being a parameter-based approach, the actual issue would be the high number of hyper-parameters, not the functional class of the policy.
-We focused on discretization in this work, but global optimization techniques could be employed as suggested by [2]. This would be future-work material.
-We think diminishing our analysis as a "theoretical exercise" is unfair. Although the regret analysis is rather standard, the problem setting is peculiar because of the special correlation among arms, requiring the usage of tools like MIS. In fact, although this may not be completely clear from the paper (however, cf the last paragraph of Section 4), it was studying the problem of exploration in policy search that we identified this kind of correlation, and built the MAB framework as a way to explicitly exploit it (and not viceversa). Thus, OPTIMIST should not be seen an application of a MAB framework to policy search, rather as a way to formulate exploration in policy search as a MAB-like problem. As pointed out by R4, ours is the first work that applies OFU to policy search while exploiting this shared information.
-We agree that the paragraph starting from line 146 (left) could give the wrong impression that the variance of balance heuristic is the same as the one of mixture sampling. The true relation is made explicit in the proof of Lemma 1. We will clarify this in the main paper too.
-Related work: adding more statistical sources would be definitely good. The ones suggested by R3 seem a good starting point.
-Experiments: we agree including PGPE in the regret plot would be useful. Plotting the regret for the MC experiment would require to know the performance of the optimal policy. Studying why PGPE gives better results (despite not having any guarantees) is an interesting future direction. However, being this a first work on regret minimization in PO, we think the focus should be more on theory than on practical performance.

R2
2.This kind of regret was achieved in the MAB literature, but is new for the policy optimization setting.
3.This is why we focus on discretization approaches, but we plan to use global optimization methods [2] in future work.
4.Do you have any specific suggestions on how to improve the experimental section?
5.We will add more trials in the final version.

R3
-We will revise Section 2.1, for the final version, to make it clearer. Are there some specific unclear point?
-We thank the reviewer for suggesting additional related works, we will include them in the final version. The one by Elvira et al. seems of practical relevance for our work, as scalability when the number of proposal distribution grows is an important issue.

R4
Thank you for pointing out typos.

[1] Gil, Manuel, Fady Alajaji, and Tamas Linder. "RÃ©nyi divergence measures for commonly used univariate continuous distributions." Information Sciences 249 (2013): 124-131.
[2] Srinivas, Niranjan, et al. "Gaussian process optimization in the bandit setting: No regret and experimental design." arXiv preprint arXiv:0912.3995 (2009).
