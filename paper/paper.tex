\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

%Custom packages and macros
\usepackage{mymacros}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsthm,thmtools,thm-restate}
\usepackage{mathabx}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{nicefrac}

\makeatletter
\newcommand{\vast}{\bBigg@{3}}
\makeatother

%Notes
\usepackage[colorinlistoftodos, textsize=tiny]{todonotes}
\definecolor{citrine}{rgb}{0.89, 0.82, 0.04}
\definecolor{blued}{RGB}{70,197,221}
%%todo by Marcello
\newcommand{\todomarc}[1]{\todo[color=orange, inline]{MARCELLO: #1}}
\newcommand{\todomarcout}[1]{\todo[color=orange]{M: #1}}
%%todo by Matteo
\newcommand{\todomat}[1]{\todo[color=green, inline]{MATTEO: #1}}
\newcommand{\todomatout}[1]{\todo[color=green]{M: #1}}
%%todo by Alberto
\newcommand{\todoalb}[1]{\todo[color=blued, inline]{ALBERTO: #1}}
\newcommand{\todoalbout}[1]{\todo[color=blued]{A: #1}}
%%todo by Lorenzo
\newcommand{\todolor}[1]{\todo[color=yellow, inline]{LORENZO: #1}}
\newcommand{\todolorout}[1]{\todo[color=yellow]{L: #1}}

%Theorems
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{sublemma}{Lemma}[section]
\newtheorem{assumption}{Assumption}

%Specific macros
\DeclareRobustCommand{\algoname}{OPOMIS\@\xspace}

\allowdisplaybreaks[4]

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Short Title}

\begin{document}

\twocolumn[
\icmltitle{Optimistic Policy Optimization via Multiple Importance Sampling}

%TITLE IDEAS
	% Optimistic Policy Search/Optimization via Multiple Importance Sampling
	% Policy Exploration via Multiple Importance Sampling
	% Upper Confidence Policy Optimization/Search

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,poli}
\icmlauthor{Bauiu C.~Yyyy}{equal,poli}
\icmlauthor{Cieua Vvvvv}{poli}
\icmlauthor{Iaesut Saoeu}{poli}
\end{icmlauthorlist}

\icmlaffiliation{poli}{Politecnico di Milano, Milan, Italy}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\lipsum[1]
\end{abstract}

\section{Introduction}
\begin{itemize}
	\item Policy Search
	\item Exploration-exploitation tradeoff
	\item Bandits and OFU
	\item Exploration in RL and PS
	\item PS as a bandit problem
	\item Existing work on infinite arms
	\item Exploiting structure with MIS
	\item Heavy tails
	\item Contributions
	\item Structure
\end{itemize}

\section{Preliminaries}
In this section we provide an essential background on policy optimization and multiple importance sampling.
\subsection{Policy optimization}
In Policy Optimization~\citep{deisenroth2013survey} we seek the policy maximizing the agent's performance on a given RL task. The task is modeled as a continuous Markov Decision Process~\citep[MDP,][]{puterman2014markov} $\Task = \langle\Sspace,\Aspace,\Tran,\Rew,\gamma,\init\rangle$, where $\Sspace\in\Reals^{d_{\Sspace}}$ is the state space; $\Aspace\in\Reals^{d_{\Aspace}}$ is the action space; $\Tran:\Sspace\times\Aspace\to\Delta(\Sspace)$ is a Markovian transition kernel, such that, for each time $h$, the next state is drawn as $s_{h+1}\sim\Tran(\cdot|s_h,a_h)$ depending only on the current state and action; $\Rew:\Sspace\times\Aspace\to[-\Rmax,\Rmax]$ is a bounded reward signal, such that the next reward $r_{h+1} = \Rew(s_h,a_h)$ is a function of the current state and action, and $\Rmax>0$ is the maximum reward; $\gamma\in(0,1]$ is a discount factor; and $\init\in\Delta(\Sspace)$ is the initial state distribution, such that the initial state is drawn as $s_0\sim\mu$. The agent's behavior is modeled as a parametric policy $\pi_{\vtheta}:\Sspace\to\Delta(\Aspace)$, such that the current action is drawn as $a_h\sim\pi_{\vtheta}(\cdot|s_h)$ depending on the current state, where $\vtheta\in\Theta\subseteq\Reals^m$ are the policy parameters. Deterministic policies represent a special case where $\pi_{\vtheta}$ is a Dirac delta function. With abuse of notation, we write $a_h=\pi_{\vtheta}(s_h)$ in this case. We focus on the episodic setting, where the agent's experience is organized into finite trajectories of maximum length $H$, called the task's horizon\footnote{This is \wlg provided that the horizon $H$ is sufficiently long for the agent to reach steady optimal behavior}. A trajectory is a sequence of states and actions $\tau=[s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1}]$. Every policy $\pi_{\vtheta}$ induces a distribution over trajectories, whose density is denoted as $p_{\vtheta}$. Our basic measure of performance is the sum of discounted rewards over the trajectory:
\begin{align}\label{eq:Rtau}
	\Rew(\tau) = \sum_{h=0}^{H-1}\gamma^hr_{h+1}.
\end{align}
Let $J(\vtheta) = \Exp_{\tau\sim p_{\vtheta}}[\Rew(\tau)]$ be the expected performance under policy $\pi_{\vtheta}$. In an \textit{online learning} scenario, we aim to maximize the sum of expected performances over a sequence of episodes $t=0,\dots,T$. 
In the \textit{action-based} policy optimization paradigm~\citep{peters2008reinforcement}, the problem we want to solve is simply:
\begin{align}\label{eq:Jtheta}
	\max_{\vtheta_0,\dots,\vtheta_T\in\Theta} \Exp_{\tau_t\sim p_{\vtheta_t}}\left[\sum_{t=0}^T\Rew(\tau_t)\right] = 
	\max_{\vtheta_0,\dots,\vtheta_T\in\Theta} \sum_{t=0}^TJ(\vtheta_t),
\end{align}
where $\pi_{\vtheta_t}$ is the policy used for episode $t$.
In the action-based paradigm, stochastic policies are typically employed in order to ensure exploration, although deterministic policies have also been used with the addition of exogenous noise~\citep{silver2014deterministic}.
Instead, in the \textit{parameter-based} policy optimization paradigm~\citep{sehnke2008policy}, we define a distribution over policy parameters, $\nu_{\vxi}\in\Delta(\Theta)$, called \textit{hyperpolicy}, where $\vxi\in\Xi\subseteq\Reals^d$ are the hyperpolicy parameters, or \textit{hyperparameters}. For each episode $t$, policy parameters are drawn as $\vtheta_t\sim\nu_{\vxi_t}$ and the whole trajectory is executed with $\pi_{\vtheta_t}$. In this case, the optimization problem becomes:
\begin{align}\label{eq:Jx}
	\max_{\vxi_0,\dots,\vxi_T\in\Xi} \sum_{t=0}^T\Exp_{\vtheta_t\sim\nu_{\vxi_t}}\left[J(\vtheta_t)\right].
\end{align}
In the parameter-based paradigm, deterministic policies are typically employed, paired with stochastic hyperpolicies in order to ensure exploration.

\subsection{Multiple importance sampling}
Importance sampling~\citep{cochran2007sampling,mcbook} is a technique that allows to estimate the expectation of a function under some \textit{target} or \textit{proposal} distribution with samples drawn from a different distribution, called \textit{behavioral}. Multiple importance sampling~\citep{veach_optimally_1995} is a generalization of this technique that allows to employ, for the same estimation, samples drawn from several different behavioral distributions. Let $P$ and $Q$ be probability measures on a measurable space $(\mathcal{Z}, \mathcal{F})$, such that $P\ll Q$ (\ie $P$ is absolutely continuous \wrt $Q$). The importance weight $w_{P/Q}$ is the Radon-Nikodym derivative of $P$ \wrt $Q$, \ie~$w_{P/Q} \equiv \frac{\de P}{\de Q}$. Let $p$ and $q$ be the densities of $P$ and $Q$, respectively, \wrt a reference measure. From the chain rule, $w_{P/Q} = \frac{p}{q}$. In the continuous case, $p$ and $q$ are probability density functions (pdf's) of continuous random variables having laws $P$ and $Q$, respectively, and $w_{P/Q}$ is a likelihood ratio. 
Given a bounded function $f:\mathcal{Z}\to\Reals$, and a set of \iid outcomes $z_1,\dots,z_N$ sampled from $Q$, the importance sampling estimator of $\mu\coloneqq\Exp_{z\sim p}\left[f(z)\right]$ is:
\begin{align}\label{eq:ise}
	\wh{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N}f(z_i)w_{P/Q}(z_i),
\end{align}
which is an unbiased estimator, \ie ${\Exp_{z_i\simiid q}\left[\wh{\mu}_{IS}\right] = \mu}$. 

Now, let $Q_1,\dots,Q_K$ be all probability measures over the same probability space as $P$, and $P\ll Q_k$ for $k=1,\dots,K$. Let $\beta_1(z),\dots,\beta_K(z)$ be mixture weights, \ie for all $z\in\mathcal{Z}$, ${\beta_1(z)+\dots+\beta_K(z) = 1}$ and $\beta_k(z)\geq0$ for ${k=1,\dots,K}$. Let $z_{ik}$ denote the $i$-th sample drawn from $Q_k$. Given $N_k$ \iid samples from each $q_k$, the Multiple Importance Sampling estimator (MIS) is:
\begin{align}\label{eq:mise}
	\wh{\mu}_{\text{MIS}} = \sum_{k=1}^K\frac{1}{N_k}\sum_{i=1}^{N_k}\beta_k(z_{ik})w_{P/Q_k}(z_{ik})f(z_{ik}),
\end{align}
which is also an unbiased estimator of $\mu$ for any valid choice of the mixture weights. A common choice of the mixture weights having desirable variance properties is the balance heuristic~\citep{veach_optimally_1995}: 
\begin{align}\label{eq:bhw}
	\beta_k(z) = \frac{N_kq_k(z)}{\sum_{j=1}^{K}N_jq_j(z)},
\end{align}
which yields the Balance Heuristic estimator (BH):
\begin{align}\label{eq:bhe}
	\wh{\mu}_{\text{BH}} = \sum_{k=1}^K\sum_{i=1}^{N_k}\frac{p(z_{ik})}{\sum_{j=1}^K N_jq_j(z_{ik})}f(z_{ik}).
\end{align}
Since (\ref{eq:bhw}) are valid mixture weights, $\wh{\mu}_{\text{BH}}$ is an unbiased estimator of $\mu$. Moreover, its variance is not significantly larger than any other choice of the mixture weights~\citep[][Theorem 1]{veach_optimally_1995}.

To further characterize the variance of this estimator, we need the concept of \Renyi divergence. Given probability measures $P$ and $Q$ on $(\mathcal{Z},\mathcal{F})$, where $P\ll Q$ and $Q$ is $\sigma$-finite, the $\alpha$-\Renyi divergence is defined as:
\begin{align}\label{eq:renyi}
	D_{\alpha}(P\|Q) = \frac{1}{\alpha-1}\log\int_{\mathcal{Z}}\left(w_{P/Q}\right)^{\alpha}\de Q,
\end{align}
for $\alpha\in[0,\infty]$\footnote{The special cases $\alpha=0,1$ and $\infty$ are defined by taking limits.}.
We denote with $d_{\alpha}(P\|Q) = \exp\{D_{\alpha}(P\|Q)\}$ the exponentiated $\alpha$-\Renyi divergence. Of particular interest is $D_2$, as the variance of the importance weight is $\Var_{z\sim q}\left[w_{P/Q}(z)\right] = d_2(P\|Q) - 1$, which is a divergence itself~\citep{cortes2010learning}. For this reason, we always mean the $2$-\Renyi divergence when omitting the order $\alpha$. The \Renyi divergence was used by~\citet[][Lemma 4.1]{metelli2018policy} to upper bound the variance of the importance sampling estimator as $\Var_{z_i\simiid q}\left[\wh{\mu}_{\text{IS}}\right]\leq \norm[\infty]{f}^2d_2(P\|Q)/N$. A similar result can be derived for the BH estimator:
%
\begin{restatable}{lemma}{misevarbound}\label{lem:misevarbound}
	Let $P$ and $Q_k$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and $d_2(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals$ be a bounded function, \ie $\norm[\infty]{f}<\infty$. Let $\wh{\mu}_{\text{BH}}$ be the balance heuristic estimator of $f$, as defined in (\ref{eq:bhe}), using $N_k$ \iid samples from each $Q_k$. Then, the variance of $\wh{\mu}_{\text{BH}}$ can be upper bounded as:
	\begin{align*}
		\Var_{z_{ik}\simiid Q_k}\left[\wh{\mu}_{\text{BH}}\right] \leq \norm[\infty]{f}^2\frac{d_2(P\|\Phi)}{N},
	\end{align*}
	where ${N=\sum_{k=1}^{K}N_k}$ is the total number of samples and ${\Phi=\sum_{k=1}^K\frac{N_k}{N}Q_k}$ is a finite mixture.
\end{restatable}
%

\section{Robust Importance Weighted Estimation}\label{sec:robust}
In this section, we discuss how to perform a robust importance weighting estimation. It has been recently observed that, in many cases of interest, the plain estimator~\eqref{eq:ise} has problematic tail behavior~\cite{metelli2018policy}, preventing the use of exponential concentration inequalities.\footnote{Unless we require that $d_{\infty}(P \| \Phi)$ is finite, \ie that the importance weight have finite supremum, there always exists a value $\alpha>1$ such that $d_{\alpha}(P \| \Phi)=+\infty$.} A common heuristic to address this problem consists in truncating the weight to prevent it from assuming too large values~\cite{ionides2008truncated}:
\begin{align}\label{eq:truncatedise}
	\widecheck{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N} \min \left\{ M, {w}_{P/Q}(z_i) \right\} f(z_i),
\end{align}
where $\widecheck{w}_{P/Q}^M(z_i) = $ and $M_N$ is a threshold to limit the magnitude of the importance weight. Similarly, for the multiple importance sampling case, restricting to the BH, we have:
\begin{align}\label{eq:truncatedmise}
	\widecheck{\mu}_{\text{BH}} =\frac{1}{N} \sum_{k=1}^K\sum_{i=1}^{N_k} \min \left\{M,  \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} \right\} f(z_{ik}).
\end{align}
Clearly, since we are changing the importance weights we introduce a bias term, but reducing the range of the estimation we likely achieve a benefit in terms of variance. In the following, we present the bias-variance analysis of the estimator and we conclude by showing that we are able, using an adaptive truncation, to guarantee an exponential concentration.

\begin{restatable}{lemma}{truncatedbias}\label{lem:truncatedbias}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{\ge 0}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. Then, the bias of $\widecheck{\mu}_{\text{BH}}$ can be upper bounded as:
	 \begin{equation}
	 \label{eq:biastruncated}
         0 \le \mu - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty} M^{-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon},
    \end{equation}
    and the variance of $\widecheck{\mu}_{\text{BH}}$ can be bounded as:
    \begin{equation}
    \label{eq:variancetruncated}
         \Var_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty}^2 M^{1-\epsilon} \frac{d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon}}{N},
    \end{equation}
	where ${N=\sum_{k=1}^{K}N_k}$ is the total number of samples and ${\Phi=\sum_{k=1}^K\frac{N_k}{N}Q_k}$ is a finite mixture.
\end{restatable}
%
It is worth noting that, when selecting $\epsilon=1$, equation~\eqref{eq:variancetruncated} reduces to Lemma~\ref{lem:misevarbound}, as the truncation operation can only reduce the variance. Clearly, the smaller we choose $M$ the larger would be the bias. Overall, we are interested in mining the joint contribution of bias and variance. Keeping $P$ and $\Phi$ fixed we observe that the bias depend on $M$ only, whereas the variance depends on $M$ and on the number of samples $N$. Intuitively, we may allow larger truncation thresholds $M$ as the number of samples $N$ increases. The following result states that, when using an \emph{adaptive threshold} depending on $N$, we are able to reach exponential concentration.
\begin{restatable}{theorem}{thrucatedconcentration}\label{lem:thrucatedconcentration}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{\ge 0}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. 
	Let $M_N = \left( \frac{N d_{1+\epsilon}\left( P \| \Phi  \right)^{\epsilon} }{\log \frac{1}{\delta}} \right) ^{\frac{1}{1+\epsilon}}$, then with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.1}
        \widecheck{\mu}_{\text{BH}} \le \mu + \|f\|_{\infty} \left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},
    \end{equation}
    and also, with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.2}
        \widecheck{\mu}_{\text{BH}} \ge \mu - \|f\|_{\infty} \left(\sqrt{2} + \frac{4}{3} \right) \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}}.
    \end{equation}
\end{restatable}

Our adaptive truncation approach and the consequent concentration results resemble the ones proposed in~\cite{bubeck2013bandits}. However, differently from~\cite{bubeck2013bandits} we do not remove sample with too high value, but we exploit the nature of the importance weighted estimator to just limit the weight magnitude. Indeed, this form of truncation turned out to be very effective in practice.


\section{Algorithm}
In this section, we use the mathematical tools presented so far to design a policy search algorithm that efficiently explores the space of solutions. The proposed algorithm, \algoname, is based on the Optimism in the Face of Uncertainty (OFU) principle and follows the Upper Confidence Bound (UCB) strategy~\citep{lai1985asymptotically,agrawal1995sample,auer2002finite} commonly used in Multi Armed Bandit (MAB) problems~\citep{robbins1985some,bubeck2012regret,lattimore2019bandit}. 

The problem we want to solve does \textit{not} fall into the MAB framework (not into its basic version, anyway) and can benefit from an ad-hoc formalization. Let $\Xspace\subseteq \Reals^d$ be our decision set, or \textit{arm set} in MAB jargon. Let $(\Omega,\mathcal{F},P)$ be a probability space. Let ${\{Z_{\vx}:\Omega\to\mathcal{Z}\mid \vx\in\Xspace\}}$ be a set of continuous random vectors parametrized by $\Xspace$, with common sample space ${\mathcal{Z}\subseteq \Reals^m}$. We denote with $p_{\vx}$ the probability density function of $Z_{\vx}$. Finally, let $f:\mathcal{Z}\to\Reals$ be a bounded \textit{payoff function}, and $\mu(\vx) = \Exp_{z\sim p_{\vx}}\left[f(z)\right]$ its expectation under $p_{\vx}$. For each iteration $t=0,\dots,T$, we select an arm $\vx_t$, draw a sample $z_t$ from $p_{\vx_t}$, and observe payoff $f(z_t)$, up to horizon $T$. The goal is to maximize the expected total payoff:
\begin{align}\label{eq:theproblem}
	\max_{\vx_0,\dots,\vx_T\in\Xspace} \Exp_{z_t\sim p_{\vx_t}}\left[\sum_{t=0}^T f(z_t)\right] = \max_{\vx_0,\dots,\vx_T\in\Xspace}\sum_{t=0}^{T}\mu(\vx_t).
\end{align}
Although we can evaluate $p_{\vx}$ for every $\vx\in\Xspace$, we can only observe $f(z_t)$ for the $z_t$ that are actually sampled. 
This models precisely the online, episodic policy optimization problem.
In action-based policy optimization, $\Xspace$ corresponds to the parameter space $\Theta$ of a class of stochastic policies ${\{\pi_{\vtheta}\mid\vtheta\in\Theta\}}$, $\mathcal{Z}$ to the set of possible trajectories, $p_{\vx}$ to the density $p_{\vtheta}$ over trajectories induced by policy $\pi_{\vtheta}$, and $f(z)$ to cumulated reward $\Rew(\tau)$. 
In parameter-based policy optimization, $\Xspace$ corresponds to the hyperparameter space $\Xi$ of a class of stochastic hyperpolicies $\{\nu_{\vxi}\mid\vxi\in\Xi\}$, $\mathcal{Z}$ to policy parameter space $\Theta$, $p_{\vx}$ to hyperpolicy $\nu_{\xi}$, and $f(z)$ to performance $J(\vtheta)$. In both cases, each iteration corresponds to a single episode, and horizon $T$ is the total number of episodes (not to be confused with the trajectory horizon $H$). From now on, we will refer to (\ref{eq:theproblem}) simply as the policy optimization problem. The peculiarity of this framework, \wrt classic MAB, is the special structure existing over the arms. In particular, the expected payoff $\mu$ of different arms is correlated thanks to the stochasticity of the $p_{\vx}$'s on a common sample space $\mathcal{Z}$.

To apply the UCB strategy to the policy optimization problem, we need an estimate of the objective $\mu(\vx)$ and a confidence region. We use importance sampling to capture the correlation among the arms. In particular to better use all the data that we collect, we would like to use a multiple importance sampling estimator like the one from (\ref{eq:mise}). Unfortunately, the heavy-tailed behavior of this estimator would result in inefficient exploration.
Instead, we use the robust balance heuristic estimator $\wc{\mu}_{\text{BH}}$ from (\ref{eq:truncatedmise}), which has better tail behavior. To simplify the notation, \wlg, we treat each sample $\vx$ as a distinct one. This corresponds to the case $K=t-1$ and $N_k\equiv1$. Hence, at each iteration $t$:
\begin{align}\label{eq:wcmu}
	\wc{\mu}_t(\vx) = \sum_{k=0}^{t-1}
	\min\left\{M_{t-1}, \frac{p_{\vx}(z_k)}
	{\sum_{j=1}^{t-1}p_{\vx_j(z_k)}}\right\}f(z_k),
\end{align}
where ${M_{t} = \left(\frac{td_{1+\epsilon}(p_{\vx}\|\Phi_t)^{\epsilon}}{\log\frac{1}{\delta_t}}\right)^{\frac{1}{1+\epsilon}}}$ and ${\Phi_t = \frac{1}{t}\sum_{k=0}^{t-1}p_{\vx_k}}$.
According to Theorem \ref{lem:thrucatedconcentration}, the following \textit{index}:
\begin{align}
	&B_t(\vx,\delta_t) \coloneqq 
	\wc{\mu}_t(\vx) \nonumber\\
	&\quad+\norm[\infty]{f}\left(\sqrt{2}+\frac{4}{3}\right)
	\left(\frac{d_{1+\epsilon}(p_{\vx_t}\|\Phi_{t})\log\frac{1}{\delta_t}}{t}\right)^{\frac{\epsilon}{1+\epsilon}},
\end{align}
is an upper bound on $\mu(\vx)$ with probability at least $1-\delta_t$, \ie an upper confidence bound. The \algoname algorithm simply selects, at each iteration $t$, the arm with the largest value of the index $B_t(\vx)$. Pseudocode is provided in Algorithm \ref{alg:1}. The initial arm $\vx_0$ is arbitrary, as no prior information is available. The regret analysis of Section \ref{sec:regret} will provide a confidence schedule $(\delta_t)_{t=1}^T$. Knowledge of the actual horizon $T$ is not needed. Although any $\epsilon>0$ works, we suggest to use $\epsilon=1$ in practice, as it yields the more common $2$-\Renyi divergence. To be able to compute the indexes (or to perform any kind of index maximization), the algorithm needs to store all the $\vx_t$ together with the observed payoffs $f(z_t)$, hence $\mathcal{O}(Td)$ space is required, where $d$ is the dimensionality of the arm space $\Xspace$ (not to be confused with cardinality $|\Xspace|$, which may be infinite). The optimization step (line 4) may be very difficult when $\Xspace$ is not discrete~\citep[\cf][]{srinivas2009gaussian}. As often done in RL, we assume the time spent collecting samples dominates computational time. Recall that drawing a sample $z_t$ in Algorithm \ref{alg:1} corresponds to executing an entire trajectory of experience, which may be very time-consuming in real-world applications. Practical aspects of \algoname will be further discussed in Section \ref{sec:practical}.

\begin{algorithm}[t]
	\caption{\algoname}
	\label{alg:1}
	\begin{algorithmic}[1]
	\STATE {\bfseries Input:} initial arm $\vx_0$, confidence schedule $(\delta_t)_{t=1}^T$
	\STATE Draw sample $z_0\sim p_{\vx_0}$ and observe payoff $f(z_0)$
	\FOR{$t=1,\dots,T$}
		\STATE Select arm $\vx_t = \arg\max_{\vx\in\Xspace}B_t(\vx,\delta_t)$
		\STATE Draw sample $z_t\sim p_{\vx_t}$ and observe payoff $f(z_t)$
	\ENDFOR
	\end{algorithmic}
\end{algorithm}

\section{Regret Analysis}\label{sec:regret}
In this section, we provide high-probability guarantees on the quality of the solution provided by Algorithm \ref{alg:1}.
First, we rephrase the optimization problem (\ref{alg:1}) in terms of \textit{regret minimization}. The instantaneous regret is defined as:
\begin{align}\label{eq:inregret}
	\Delta_t = \mu(\vx^*) - \mu(\vx_t),
\end{align}
where $\vx^* = \arg\max_{\vx\in\Xspace}\mu(\vx)$. Let $\Reg(T) = \sum_{t=0}^T\Delta_t$ be the total regret.
As $\mu(\vx^*)$ is a constant, problem (\ref{eq:theproblem}) is trivially equivalent to:
\begin{align}\label{eq:regret}
	\min_{\vx_0,\dots,\vx_T\in\Xspace} \Reg(T).
\end{align}
In the following, we will show that Algorithm \ref{alg:1} yields sublinear regret under some mild assumptions. The proofs combine techniques from~\citet{srinivas2009gaussian} and~\citet{bubeck2013bandits} and are reported in Appendix \ref{app:proof}.
First, we need the following assumption on the \Renyi divergence:
%
\begin{restatable}{assumption}{boundrenyi}\label{ass:boundrenyi}
	For all $t=1,\dots,T$, the $(1+\epsilon)$-\Renyi divergence is uniformly bounded as:
	\begin{align*}
		\sup_{\vx_0,\vx_1,\dots,\vx_t\in\Xspace}d_{1+\epsilon}(p_{\vx_t}\|\Phi_t) = v_{\epsilon} < \infty,
	\end{align*}
	where $\Phi_t = \frac{1}{t}\sum_{k=0}^{t-1}p_{\vx_k}$,
\end{restatable}
%
which can be easily enforced through careful policy (or hyperpolicy) design. Next, we need some assumptions on the structure of the arm set $\Xspace$.

\subsection{Discrete arm set}
We start from the discrete case, where $|\mathcal{X}| = K \in \Naturals_{+}$.
This setting is particularly convenient, as the optimization step can be trivially solved in time $\mathcal{O}(Kt^2)$ per iteration\footnote{We consider the evaluation of pdf's, payoffs and \Renyi divergences in (\ref{eq:wcmu}) atomic, as it is heavily problem-dependent.}, where $t^2$ is from evaluating the nested sums in (\ref{eq:wcmu}). This sums up to total time $\mathcal{O}(KT^3)$. It is also of practical interest: even in applications where $\Xspace$ is naturally continuous (\eg robotics), the set of solutions that can be actually tried in practice may sometimes be constrained to a discrete, reasonably small set. In this simple setting, \algoname achieves $\wt{\mathcal{O}}(T^{\frac{1}{1+\epsilon}})$ regret:

\begin{restatable}{theorem}{regretdiscrete}\label{th:regretdiscrete}
	Let $\Xspace$ be a discrete arm set with ${|\mathcal{X}| = K \in \Naturals_{+}}$. Under Assumption \ref{ass:boundrenyi}, Algorithm \ref{alg:1} with confidence schedule $\delta_t = \frac{3\delta}{t^2\pi^2K}$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
		&\Reg(T) \leq \Delta_0 \\
		&\quad+ 	C
			T^{\frac{1}{1+\epsilon}}
			\left[v_{\epsilon}
			\left(2\log T + \log \frac{\pi^2K}{3\delta}\right)
			\right]^{\frac{\epsilon}{1+\epsilon}},
	\end{align*}
	where $C=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
This yields a $\wt{\mathcal{O}}(\sqrt{T})$ regret when $\epsilon=1$.

\subsection{Compact arm set}
Now, we consider the more general case of a compact arm set $\Xspace\in\Reals^d$. This case is also more interesting as it allows to tackle virtually any RL task. We can assume, \wlg, that $\Xspace$ is entirely contained in a box $[-D,D]^d$, with $D\in\Reals_{+}$. We also need the following assumption on the expected payoff:
%
\begin{restatable}{assumption}{lipschitz}\label{ass:lipschitz}
	The expected payoff $\mu$ is Lipschitz continuous, \ie, there exists a constant $L>0$ such that, for every $\vx, \vx'\in\Xspace$:
	\begin{align*}
		|\mu(\vx') - \mu(\vx)| \leq L\norm[1]{\vx - \vx'}.
	\end{align*}
\end{restatable}
%
This assumption is easily satisfied for policy optimization, as shown in the following:
%
\begin{restatable}{lemma}{lipschitzpol}\label{lem:lispschitzpol}
	In the policy optimization problem, Assumption \ref{ass:lipschitz} can be replaced by:
	\begin{align}\label{eq:lp1}
		\sup_{s\in\Sspace,\vtheta\in\Theta}\Exp_{a\sim\pi_{\vtheta}}
		\left[\left|\nabla_{\vtheta}\log\pi_{\vtheta}(a|s)\right|\right] \leq \vu_1,
	\end{align}
	in the action-based paradigm, and by:
	\begin{align}\label{eq:lp2}
		\sup_{\vxi\in\Xi}\Exp_{\vtheta\sim\rho_{\vxi}}
		\left[\left|\nabla_{\vxi}\log\rho_{\vxi}(\vtheta)\right|\right] \leq \vu_2,
	\end{align}
	in the parameter-based paradigm, where $\vu_1$ and $\vu_2$ are $d$-dimensional vectors and the inequalities are component-wise.
\end{restatable}
%
In the proof, we show how to derive the corresponding Lipschitz constants, and show how (\ref{eq:lp1}) and (\ref{eq:lp2}) are satisfied by the commonly-used Gaussian policy and hyperpolicy, respectively. This is enough for \algoname to achieve $\wt{\mathcal{O}}(d^{\frac{\epsilon}{1+\epsilon}}T^{\frac{1}{1+\epsilon}})$ regret:
%
\begin{restatable}{theorem}{regretcompact}\label{th:regretcompact}
	Let $\Xspace$ be a $d$-dimensional compact arm set with $\Xspace \subseteq [-D,D]^d$. Under Assumptions \ref{ass:boundrenyi} and \ref{ass:lipschitz}, Algorithm \ref{alg:1} with confidence schedule $\delta_t = \frac{6\delta}{\pi^2t^2(1+d^dt^{2d})}$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
	&\Reg(T) \leq \Delta_0 \\
	&\quad+ 	C
	T^{\frac{1}{1+\epsilon}}
	\left[v_{\epsilon}
	\left(2(d+1)\log T + d\log d + \log \frac{\pi^2}{3\delta}\right)
	\right]^{\frac{\epsilon}{1+\epsilon}} \\
	&\quad+ \frac{\pi^2LD}{6},
	\end{align*}
	where $C=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
%
This yields a $\wt{\mathcal{O}}(\sqrt{dT})$ regret when $\epsilon=1$.
\section{Practical Aspects}\label{sec:practical}
In this section, we discuss some practical aspects of \algoname.

\subsection{Optimization}
Some of the most relevant applications of policy search (\eg robotics) naturally involve infinite arm sets $\Xspace$. As mentioned before, optimization (line 4 in Algorithm \ref{alg:1}) can be very challenging in this scenario, as the index $B_t(\vx,\delta_t)$ is non-convex and non-differentiable. Global optimization methods could be applied at the cost of giving up theoretical guarantees. This direction may be beneficial in practice, but we leave it to future, more application-oriented work. Instead, we propose a general discretization method, inspired by the proof of Theorem \ref{th:regretcompact}, which preserves the regret bound for the compact case. The key intuition, common in the continuous MAB literature, is to make the discretization progressively finer. The pseudocode for this variant, called \algoname2, is reported in Algorithm \ref{alg:2}. Note that the arm space $\Xspace$ itself is fixed (and infinite), as the adaptive discretization is performed for optimization purposes only. A reasonably coarse discretization can be used at the price of a worse (sublinear) regret:
%
\begin{restatable}{theorem}{regretdiscretized}\label{th:regretdiscretized}
	Let $\Xspace$ be a $d$-dimensional compact arm set with $\Xspace \subseteq [-D,D]^d$. For any $\kappa\geq2$, under Assumptions \ref{ass:boundrenyi} and \ref{ass:lipschitz}, Algorithm \ref{alg:2} with confidence schedule ${\delta_t = \frac{6\delta}{\pi^2t^2\left(1+\left\lceil t^{\nicefrac{1}{\kappa}}\right\rceil^d\right)}}$ and discretization schedule $\tau_t=\lceil t^{\frac{1}{\kappa}} \rceil$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
	&\Reg(T) \leq \Delta_0  + C_1T^{\left(1-\frac{1}{\kappa}\right)}d
	+ C_2
	T^{\frac{1}{1+\epsilon}} \,\cdot
	\\&\quad\cdot\left[v_{\epsilon}
	\left(\left(2+ \nicefrac{d}{\kappa}\right)\log T + d\log 2 + \log\frac{\pi^2}{3\delta}\right)\right]^{\frac{\epsilon}{1+\epsilon}},
	\end{align*}
	where $C_1=\frac{\kappa}{\kappa-1}LD$, $C_2=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
%
From now on, we will focus on the case $\epsilon=1$, which is the only one of practical interest in the scope of this paper.
For $\kappa=2$, we obtain regret $\wt{\mathcal{O}}(d\sqrt{T})$. Unfortunately, the time required for optimization is exponential in arm space dimensionality $d$. For $d\geq 2$, we can break the curse of dimensionality by taking $\kappa=d$. In this case, the regret is $\wt{\mathcal{O}}\left(dT^{\left(1-\frac{1}{d}\right)}\right)$. On the other hand, the time per iteration is only $O(t^3)$. Note that the regret is sublinear for any choice of $\kappa$. 
Going further: for any $\zeta>0$, $\kappa=\frac{d}{\zeta}$ grants $O(t^{2+\zeta})$ time per iteration at the cost of $\wt{\mathcal{O}}\left(dT^{\left(1-\frac{\zeta}{d}\right)}\right)$ regret\footnote{The worse dependency $\mathcal{O}(d)$ of the regret on the arm space dimensionality (\wrt $\mathcal{O}(\sqrt{d})$ of Algorithm \ref{alg:1}) is also necessary to prevent the time per iteration from being exponential in $d$.}. 

\begin{algorithm}[t]
	\caption{\algoname2}
	\label{alg:2}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:} initial arm $\vx_0$, confidence schedule $(\delta_t)_{t=1}^T$, discretization schedule $(\tau_t)_{t=1}^T$
		\STATE Draw sample $z_0\sim p_{\vx_0}$ and observe payoff $f(z_0)$
		\FOR{$t=1,\dots,T$}
		\STATE Discretize $\Xspace$ with a uniform grid $\wt{\Xspace}_t$ of $\tau_t^d$ points
		\STATE Select arm $\vx_t = \arg\max_{\vx\in\wt{\Xspace}_t}B_t(\vx,\delta_t)$
		\STATE Draw sample $z_t\sim p_{\vx_t}$ and observe payoff $f(z_t)$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
%

\subsection{Upper Bound for the Exponentiated Renyi Divergence between mixtures}
\algoname requires at each iteration to compute the exponentiated \Renyi divergence between the currently considered distribution $p_{\vx}$ 
and the mixture $\Phi_t$, \ie $td_{1+\epsilon}(p_{\vx}\|\Phi_t)$. Even for Gaussian distributions, this quantity cannot be obtained in closed form, while
the \Renyi divergence between Gaussians can be computed exactly. In this section, we provide a upper bound for computing the exponentiated \Renyi divergence
between a generic distribution and a mixture.
\begin{restatable}{theorem}{armonic}\label{th:armonic}
	Let $P$ be a probability measure and $\Phi = \sum_{k=1}^K \beta_k Q_k$, with $\beta_k \in [0,1]$ and $\sum_{k=1}^K \beta_k =1$, be a finite mixture of the
	probability measures $\{Q_k\}_{k=1}^K$. Then, for any $\alpha \ge 1$, the exponentiated $\alpha$-\Renyi divergence can be bounded as: 
    \begin{equation*}
        d_{\alpha}(P \| \Phi) \le \frac{1} {\sum_{k=1}^K \frac{ \beta_k}{ d_{\alpha}(P \| Q_k)}}.
    \end{equation*}
\end{restatable}

\subsection{Parameter-based exploration}
\begin{itemize}
	\item Bound on $d_2$
\end{itemize}
\subsection{Action-based exploration}
\begin{itemize}
	\item Estimator of $d_2$
\end{itemize}

\section{Related Works}

\section{Experiments}

\section{Conclusion}

% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}
%
%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.
%
%If a paper is accepted, the final camera-ready version can (and
%probably should) include acknowledgements. In this case, please
%place such acknowledgements in an unnumbered section at the
%end of the paper. Typically, this will include thanks to reviewers
%who gave useful comments, to colleagues who contributed to the ideas,
%and to funding agencies and corporate sponsors that provided financial
%support.

\bibliography{../biblio}
\bibliographystyle{icml2019}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUPPLEMENTARY MATERIALS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\onecolumn
\appendix
\section{Proofs}\label{app:proof}

\misevarbound*
%
\begin{proof}
	The proof is similar to Lemma 4.1 of~\cite{metelli2018policy}:
    \begin{align}
    \Var_{z_{ik} \simiid Q_k} [\widehat{\mu}_{\text{BH}}]&  = \Var_{z_{ik} \simiid Q_k} \left[ \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{N_k}  f(z_{ki})\frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right] \notag \\
    & = \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Var_{z_{ik} \sim Q_k} \left[   f(z_{ki}) \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right] \label{p:misevarboundline2} \\
    & \le \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( f(z_{ki})   \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right)^2 \right] \notag\\
    & \le \|f\|_{\infty} \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right)^2 \right]  \notag\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \left(\frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right)^2 \right] \label{p:misevarboundline5}\\
    &= \|f\|_{\infty}^2 \frac{d_{2}(P\|\Phi)}{N},  \notag
    \end{align}
    where~\eqref{p:misevarboundline2} follows from the independence of the $z_{ik}$ and~\eqref{p:misevarboundline5} is obtained by the definition of $\Phi$ and observing that for an arbitrary function $g$:
    \begin{equation}
    \label{eq:expectationdecomposition}
    	\frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k}[g(z_{ik})] =  \sum_{k=1}^K \frac{N_k}{N} \Exp_{z_{1k} \sim Q_k}[g(z_{1k})] = \Exp_{z \sim \Phi}[g(z)].
    \end{equation}
\end{proof}


\truncatedbias*
\begin{proof}
Let us start with the bias term. The first inequality $0 \le \mu - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}]$ derives from the fact that $\widehat{\mu}_{\text{BH}} \ge \widecheck{\mu}_{\text{BH}}$, being $f(z) \ge 0$ for all $z$ and observing that $\widehat{\mu}$ is unbiased, \ie $\Exp_{z_{ik} \simiid Q_k}[\widehat{\mu}_{\text{BH}}]=\mu$. For the second inequality, let us consider the following derivation:
    \begin{align}
    \mu - \Exp_{x_i \sim q_i} [\widecheck{\mu}]&  = \Exp_{z_{ik} \simiid Q_k}[\widehat{\mu}_{\text{BH}}] - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \notag \\
    & =  \frac{1}{N} \sum_{k=1}^K\sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[  f(z_{ik}) \left( \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} - \min \left\{ M,  \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} \right\} \right) \right]\\    
%    & =   \sum_{k=1}^K \frac{N_k}{N} \Exp_{z_{1k} \sim Q_k} \left[  f(z_{1k}) \left( \frac{p(z_{1k})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{1k})} - \min \left\{ M,  \frac{p(z_{1k})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{1k})} \right\} \right) \right] \\
%     & = \Exp_{z \sim \Phi} \left[  f(z) \left( \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} - \min \left\{ M,  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right\} \right) \right]\\
    & =  \Exp_{z \sim \Phi} \left[  f(z) \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} - M  \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right] \label{p:truncatedbias1}\\
    & \le \Exp_{z \sim \Phi} \left[  f(z) \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right] \label{p:truncatedbias2}\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right] \notag\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon} \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{-\epsilon} \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right] \notag\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon} \right] M^{-\epsilon} \label{p:truncatedbias3}\\
        & = \|f\|_{\infty} d_{1+\epsilon}(P \| \Phi)^{\epsilon}  M^{-\epsilon},\notag
    \end{align}
    where~\eqref{p:truncatedbias1} is an application of equation~\eqref{eq:expectationdecomposition}, \eqref{p:truncatedbias2} derives from recalling that $M\ge 0$ and \eqref{p:truncatedbias3} is obtained by observing that $ x^{-\epsilon} \mathds{1}_{\left\{ x \ge M \right\} }$ is either $0$ and thus the bound holds or at most $M^{-\epsilon}$.
    For the variance the argument is similar:
    \begin{align}
    \Var_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}]&  = \Var_{z_{ik} \simiid Q_k} \left[ \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{N_k}  f(z_{ki}) \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right] \notag\\
    & = \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Var_{z_{ik} \sim Q_k} \left[   f(z_{ki}) \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right] \notag\\
    & \le \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( f(z_{ki})  \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right)^2 \right]\notag\\
    & \le \|f\|_{\infty} \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right)^2 \right]\notag\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^2 \right] \label{p:truncatedbias4}\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^{1+\epsilon} \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^{1-\epsilon}  \right]\notag\\
    & \le \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \left( \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon}  \right] M^{1-\epsilon} \label{p:truncatedbias5}\\
    &= \|f\|_{\infty}^2 M^{1-\epsilon} \frac{d_{1+\epsilon}(P\|\Phi)^{\epsilon}}{N} ,\notag\\
    \end{align}
    where~\eqref{p:truncatedbias4}  is again an application of equation~\eqref{eq:expectationdecomposition} and~\ref{p:truncatedbias5} derives from observing that $\min\{x,y\} \le x$ and also $\min\{x,y\} \le y$.
\end{proof}

\thrucatedconcentration*

\begin{proof}
Let us start with the first inequality. Observing that all samples $z_{ik}$ are independent and that $\widecheck{\mu}_{\text{BH}} \le M \|f\|_{\infty}$, we can state using Bernstein inequality~\cite{boucheron2013concentration} that with probability at least $1-\delta$ we have:
    \begin{align}
         \widecheck{\mu}_{\text{BH}} & \le \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] + \sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} +\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N} \notag \\
         & \le \mu + \|f\|_{\infty} \sqrt{\frac{2 M^{1-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon} \log  \frac{1}{\delta} }{N}} +\|f\|_{\infty} \frac{M \log \frac{1}{\delta}}{3N} \label{p:thrucatedconcentration1}\\
       & = \mu + \|f\|_{\infty}\left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}} \label{p:thrucatedconcentration2},
    \end{align}
    where~\eqref{p:thrucatedconcentration1} is obtained by substituting the variance with its bound~\eqref{eq:variancetruncated} and~\eqref{p:thrucatedconcentration2} is from the choice of $M$.
    For the second inequality we just need to consider additionally the bias.
    \begin{align}
         \widecheck{\mu}_{\text{BH}} & \ge  \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] - \sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} -\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N} \notag\\
         & = \mu - \left(\mu - \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] \right) -\sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} -\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N}  \notag\\
          & \ge \mu - \|f\|_{\infty} M^{-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon} - \|f\|_{\infty}\left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}} \label{p:thrucatedconcentration3}\\
          & = \mu - \|f\|_{\infty} \left(\sqrt{2} + \frac{4}{3} \right) \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}}, \notag\\
    \end{align}
    where~\eqref{p:thrucatedconcentration3} comes from substituting the bias with its bound~\eqref{eq:biastruncated}.
\end{proof}

%Finite arm set
\regretdiscrete*
%
\begin{proof}
	Fix an $\epsilon>0$.
	To ease the notation, let $c^{-}\coloneqq \norm[\infty]{f}\left(\sqrt{2}+\frac{1}{3}\right)$, $c^{+}\coloneqq \norm[\infty]{f}\left(\sqrt{2}+\frac{4}{3}\right)$, and $\beta_t(\vx) \coloneqq \left(\frac{d_{1+\epsilon}(p_{\vx}\|\Phi_t)\log\frac{1}{\delta_t}}{t}\right)^{\frac{\epsilon}{1+\epsilon}}$.
	We start by showing that, with probability at least $1-\delta$:
	\begin{align}\label{aux:1}
		&-c^{+}\beta_t({\vx})\leq \wc{\mu_t}(\vx) - \mu(\vx) \leq c^{-}\beta_t(\vx) & \text{for all $\vx\in\Xspace$ and $t=1,\dots,T$}.
	\end{align}
	Indeed:
	\begin{align}
		\Prob\left(\bigcap_{k=1}^{K}\bigcap_{t=1}^T\left[\wc{\mu_t}(\vx_k) - \mu(\vx_k) \leq c^{-}\beta_t(\vx_k)\right]\right) &=1-\Prob\left(\bigcup_{k=1}^{K}\bigcup_{t=1}^T\left[\wc{\mu_t}(\vx_k) - \mu(\vx_k) > c^{-}\beta_t(\vx_k)\right]\right)\nonumber\\
		&\geq 1-K\sum_{t=1}^{T}\Prob\left(\wc{\mu_t}(\vx_1) - \mu(\vx_1) > c^{-}\beta_t(\vx_1)\right) \label{pp:1}\\
		& \geq 1 - K\sum_{t=1}^{T}\delta_t\label{pp:2}\\
		& \geq 1 - \frac{\delta}{2},\label{pp:3}
	\end{align}
	where (\ref{pp:1}) is from a double union bound (over time and over the finite elements of $\Xspace$), (\ref{pp:2}) is from Theorem \ref{lem:thrucatedconcentration}, and (\ref{pp:3}) is by hypothesis on $\delta_t$ and $\sum_{t=1}^{T}\frac{1}{t^2}\leq\sum_{t=1}^{\infty}\frac{1}{t^2} = \frac{\pi^2}{6}$. Similarly:
	\begin{align*}
		\Prob\left(\bigcap_{k=1}^{K}\bigcap_{t=1}^T\left[\wc{\mu_t}(\vx_k) - \mu(\vx_k) \geq -c^{+}\beta_t(\vx_k)\right]\right) &=1-\Prob\left(\bigcup_{k=1}^{K}\bigcup_{t=1}^T\left[\wc{\mu_t}(\vx_k) - \mu(\vx_k) < -c^{+}\beta_t(\vx_k)\right]\right)\nonumber\\
		&\geq 1-K\sum_{t=1}^{T}\Prob\left(\wc{\mu_t}(\vx_1) - \mu(\vx_1) < -c^{+}\beta_t(\vx_1)\right)\\
		&\geq 1 - K\sum_{t=1}^{T}\delta_t\\
		 &\geq 1 - \frac{\delta}{2}.
	\end{align*}
	Hence, by union bound over the two inequalities, (\ref{aux:1}) holds with probability at least $1-\delta$.
	This allows to lower bound the instantaneous regret with the same probability:
	\begin{align}
		\Delta_t &= \mu(\vx^*) - \mu(\vx) \leq 
		\wc{\mu_t}(\vx^*) + c^{+}\beta_t(\vx^*) - \mu(\vx_t) \label{pp:4}\\
		&\leq \wc{\mu_t}(\vx_t) + c^{+}\beta_t(\vx_t) - \mu(\vx_t) \label{pp:5}\\
		&\leq (c^{-} + c^{+})\beta(\vx_t) & \text{for all $t=1,\dots,T$}\label{aux:2}, 
	\end{align}
	where (\ref{pp:4}) and (\ref{aux:2}) are from (\ref{aux:1}), while (\ref{pp:5}) is by hypothesis, as $\vx_t=\arg\max_{\vx\in\Xspace}(\wc{\mu_t}(\vx) + c^{+}\beta_t(\vx))$.
	Note that the union bound over the elements of $\Xspace$ in (\ref{aux:1}) was necessary for (\ref{pp:4}) as the optimal arm $\vx^*$ may not be unique.
	Finally, with probability at least $1-\delta$:
	\begin{align}
		\Reg(T) &= \sum_{t=0}^{T}\Delta_t \nonumber\\
		&= \Delta_0  + \sum_{t=1}^{T}\Delta_t \nonumber\\
		&\leq \Delta_0  + (c^{+}+c^{-})\sum_{t=1}^{T}\beta_t(\vx_t) \label{pp:6}\\
		&\leq \Delta_0  + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{\log\frac{1}{\delta_t}}{t}\right)^{\frac{\epsilon}{1+\epsilon}} \label{pp:7}\\
		&=\Delta_0  + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{2\log t + \log\frac{\pi^2K}{3\delta}}{t}\right)^{\frac{\epsilon}{1+\epsilon}} \label{pp:8}\\
		&\leq \Delta_0  + (c^{+}+c^{-})\left[v_{\epsilon}\left(2\log T + \log\frac{\pi^2K}{3\delta}\right)\right]^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}t^{-\frac{\epsilon}{1+\epsilon}} \nonumber\\
		&\leq \Delta_0  + (c^{+}+c^{-})\left[v_{\epsilon}\left(2\log T + \log\frac{\pi^2K}{3\delta}\right)\right]^{\frac{\epsilon}{1+\epsilon}}(1+\epsilon)T^{\frac{1}{1+\epsilon}} \label{pp:9},
	\end{align}
	where (\ref{pp:6}) is from (\ref{aux:2}) and holds with probability no less than $1-\delta$, (\ref{pp:7}) is from Assumption \ref{ass:boundrenyi}, (\ref{pp:8}) is by definition of $\delta_t$, and (\ref{pp:9}) is from: 
	\begin{align}\label{aux:3}
		&\sum_{t=1}^{T}t^{-\alpha} \leq \int_{1}^{T+1}t^{-\alpha}\de t
		= \frac{1}{1-\alpha}\left((T+1)^{1-\alpha} - 1\right) \leq \frac{T^{1-\alpha}}{1-\alpha} &\text{for all $0<\alpha<1$},
	\end{align}
	with $\alpha=\frac{\epsilon}{1+\epsilon}$.
	The proof is completed by renaming $C\gets (1+\epsilon)(c^{+}+c^{-}) = (1+\epsilon)(2\sqrt{2} + \frac{5}{3})\norm[\infty]{f}$.
\end{proof}

%Compact case
\lipschitzpol*
\begin{proof}
	We consider the infinite-horizon case ($H=\infty,\gamma<1$), as the finite-horizon case is \wlg under mild assumptions.
	To show Lipschitz continuity in the action-based paradigm, it is enough to bound $\norm[\infty]{\nabla_{\vtheta}J}$ under (\ref{eq:lp1}). From the Policy Gradient Theorem~\cite{sutton2000policy}:
	\begin{align}\label{aux:4}
		\nabla_{\vtheta}J(\vtheta) = \frac{1}{1-\gamma}\Exp_{\substack{s\sim \rho_{\vtheta}
		\\a\sim\pi_{\vtheta}}}\left[\nabla_{\vtheta}\log\pi_{\vtheta}(a|s)Q_{\vtheta}(s,a)\right],
	\end{align}
	where $\rho_{\vtheta}$ is the discounted state-occupancy measure under policy $\pi_{\vtheta}$ and $Q_{\vtheta}$ is the action-value function~\cite{sutton2000policy}, modeling the reward that can be obtained starting from state $s$, taking action $a$ and following $\pi_{\vtheta}$ thereafter.
	From (\ref{aux:4}), for every $\vtheta\in\Theta$:
	\begin{align}
		|\nabla_{\vtheta}J(\vtheta)| 
		&\leq \frac{\Rmax}{(1-\gamma)^2}\Exp_{\substack{s\sim \rho_{\vtheta}\\a\sim\pi_{\vtheta}}}\left[|\nabla_{\vtheta}\log\pi_{\vtheta}(s,a)|\right] \label{pp:10}\\
		&\leq \frac{\Rmax}{(1-\gamma)^2}\sup_{s\in\Sspace}\Exp_{a\sim\pi_{\vtheta}}\left[|\nabla_{\vtheta}\log\pi_{\vtheta}(s,a)|\right] \nonumber\\
		& = \frac{\vu_1\Rmax}{(1-\gamma)^2}, \label{pp:11}
	\end{align}
	where the inequalities are component-wise, (\ref{pp:10}) is from the trivial fact $\norm[\infty]{Q_{\vtheta}}\leq\frac{\Rmax}{(1-\gamma)}$, and (\ref{pp:11}) is from assumption (\ref{eq:lp1}). It follows that $L=\frac{\norm[\infty]{\vu_1}\Rmax}{(1-\gamma)^2}$ is a valid Lipschitz constant under the $l_1$ norm.
	The commonly used Gaussian policy:
	\begin{align}\label{eq:gauss}
		\pi_{\vtheta}(a|s) = \mathcal{N}(\vtheta^T\vphi(s),\sigma^2)  = \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2}\left(\frac{a-\vtheta^T\vphi(s)}{\sigma}\right)^2\right\},
	\end{align}
	where $\phi(s)$ is a vector of component-wise bounded state features, \ie $\sup_{s\in\Sspace}|\vphi(s)|\leq \vphi_{\max}$, satisfies assumption (\ref{eq:lp1}):
	\begin{align}
		\Exp_{a\sim\pi_{\vtheta}}\left[\left|\nabla_{\vtheta}\log\pi_{\vtheta}(a|s)\right|\right] 
		&=\Exp_{a\sim\pi_{\vtheta}}\left[\frac{|\vphi(s)(a-\vtheta^T\vphi(s))|}{\sigma^2}\right] \nonumber\\
		&\leq \frac{|\vphi(s)|}{\sigma}\Exp_{a\sim\pi_{\vtheta}}\left[\left|\frac{a-\vtheta^T\vphi(s)}{\sigma}\right|\right] \nonumber\\
		&\leq \frac{|\vphi(s)|}{\sqrt{2\pi}\sigma}\int_{\Reals} e^{-x^2}|x|\de x \label{pp:14}\\
		& \leq  \frac{2\vphi_{\max}}{\sqrt{2\pi}\sigma}\coloneqq \vu_1, \label{aux:5}
	\end{align}
	where inequalities are component-wise and (\ref{pp:14}) is by the substitution $x\gets \frac{a-\vtheta^T\vphi(s)}{\sigma}$. Even when $\sigma$ must be learned, proper parametrization (\eg $\sigma\propto\exp\{\vtheta\}$), together with the compactness of $\Theta$, allows to satisfy assumption (\ref{eq:lp1}).
	
	To show Lipschitz continuity for the parameter-based paradigm, it is enough to bound $\norm[\infty]{\nabla_{\vxi}\Exp_{\vtheta\sim\rho_{\vxi}}[J(\vtheta)]}$ under (\ref{eq:lp2}). For every $\vxi\in\Xi$:
	\begin{align}
		\left|\nabla_{\vxi}\Exp_{\vtheta\sim\rho_{\vxi}}[J(\vtheta)]\right| &=
		\left|\Exp_{\vtheta\sim\rho_{\vxi}}[\nabla_{\vxi}\log\rho_{\vxi}(\vtheta)J(\vtheta)]\right| \nonumber\\
		&\leq \frac{\Rmax}{(1-\gamma)}\Exp_{\vtheta\sim\rho_{\vxi}}\left[\left|\nabla_{\vxi}\log\rho_{\vxi}(\vtheta)\right|\right] \label{pp:12} \\
		&\leq \frac{\vu_2\Rmax}{(1-\gamma)} \label{pp:13},
	\end{align}
	where the inequalities are component-wise, (\ref{pp:12}) is from the trivial fact $J(\vtheta)\leq\frac{\Rmax}{1-\gamma}$, and (\ref{pp:13}) is from assumption (\ref{eq:lp2}).
	It follows that $L=\frac{\norm[\infty]{\vu_2\Rmax}}{(1-\gamma)}$ is a valid Lipschitz constant under the $l_1$ norm.
	 A Gaussian hyperpolicy $\rho_{\vxi}(\vtheta) = \mathcal{N}(\vxi,\boldsymbol{\sigma}I)$ satisfies assumption (\ref{eq:lp2}) with $\vu_2 = \frac{2}{\sqrt{2\pi}\boldsymbol{\sigma}}$. The proof of this fact is analogous to that of (\ref{aux:5}). Even when $\boldsymbol{\sigma}$ must be learned, proper parametrization (\eg $\boldsymbol{\sigma}\propto\exp\{\vxi\}$), together with the compactness of $\Xi$, allows to satisfy assumption (\ref{eq:lp2}).
\end{proof}

\regretcompact*
\begin{proof}
	Fix an $\epsilon>0$. Let $c^{-}$, $c^{+}$ and $\beta_t(\vx)$ be defined as in the proof of Theorem \ref{th:regretdiscrete}.
	The finite cardinality of $\Xspace$ allowed to perform a union bound over the arms that was crucial for the proof of Theorem \ref{th:regretdiscrete}. We cannot do the same here as $\Xspace$ has infinite cardinality. To overcome this problem, we follow the line of reasoning proposed by~\citet{srinivas2009gaussian}. First, we can say something about the arms that are actually selected by the algorithm, which are finite. From Theorem \ref{lem:thrucatedconcentration}, by a union bound over $t=1,\dots,T$, we have that, with probability at least $1-\sum_{t=1}^T\delta_t$:
	\begin{align}\label{aux:6}
		&\wc{\mu_t}(\vx_t) - \mu(\vx_t) \leq c^{-}\beta_t(\vx_t) &\text{for all $t=1,\dots,T$}.
	\end{align}
	
	We also need a specular inequality for the optimal arm. Unfortunately, we cannot assume there exists a unique optimal arm $\vx^*$\footnote{Instead, $\mu(\vx^*$) is always unique.}. Even worse, a dense set of optimal arms may exist. To overcome this problem, we introduce, \textit{only for the purposes of the proof}, a discretization of the arm space. Let $\wt{\Xspace}_t$ be a $d$-dimensional regular grid of $\tau_t^d$ vertexes, where $(\tau_t\in\Naturals_{+})_{t=1}^T$ is a discretization schedule. Let $[\vx]_t$ be the closest vertex to $\vx$ in $\wt{\Xspace}_t$. From Assumption \ref{ass:lipschitz}:
	\begin{align}\label{aux:7}
		&|\mu(\vx) - \mu([\vx]_t)| \leq L\norm[1]{\vx - [\vx]_t} \leq \frac{LDd}{\tau_t},
	\end{align}
	as each voxel of the grid has side $\frac{2D}{\tau_t}$ and no point can be further from a vertex than $d$ half-sides according to the $l_1$ norm. Now fix a $t\geq1$ and an optimal arm $\vx^*$. With probability at least $1-\delta_t$:
	\begin{align}
		\mu(\vx^*) - \wc{\mu}_t([\vx^*]_t) 
		&= \mu(\vx^*) - \mu([\vx^*]_t) + \mu([\vx^*]_t) - \wc{\mu}_t([\vx^*]_t)  \nonumber\\
		&\leq \mu([\vx^*]_t) - \wc{\mu}_t([\vx^*]_t) + |\mu(\vx^*) - \mu([\vx^*]_t)| \nonumber\\
		&\leq c^{+}\beta_t([\vx^*]_t) + \frac{LDd}{\tau_t}, \label{aux:8}
	\end{align}
	where the inequality (\ref{aux:8}) is from Theorem \ref{lem:thrucatedconcentration} and (\ref{aux:7}). Since any voxel may contain an optimal arm, we must perform a union bound over the $\lceil\tau\rceil^d$ vertexes of $\wt{\Xspace}_t$, and a subsequent one over $t,\dots,T$. Hence, with probability at least $1-\sum_{t=1}^{T}\tau_t^d\delta_t$:
	\begin{align}\label{aux:9}
		&\mu(\vx^*) - \wc{\mu}_t([\vx^*]_t) \leq c^{+}\beta_t([\vx^*]_t) + \frac{LDd}{\tau_t} &\text{for $t=1,\dots,T$ and every $\vx^*\in\arg\max_{\vx\in\Xspace}\mu(\vx)$}.
	\end{align}
	
	We can now proceed to bound the instantaneous regret. With probability at least $1-\sum_{t=1}^T\delta_t(1+\tau_t^d)$:
	\begin{align}
	\Delta_t = \mu(\vx^*) - \mu(\vx_t) &\leq 
	\wc{\mu_t}([\vx^*]_t) + c^{+}\beta_t([\vx^*]_t) + \frac{LDd}{\tau_t} - \mu(\vx_t) \label{pp:15}\\
	&\leq \wc{\mu_t}(\vx_t) + c^{+}\beta_t(\vx_t) + \frac{LDd}{\tau_t} - \mu(\vx_t) \label{pp:16}\\
		&\leq (c^{+}+c^{-})\beta_t(\vx_t) + \frac{LDd}{\tau_t}, \label{aux:10}
	\end{align}
	where (\ref{pp:15}) is from (\ref{aux:9}) and holds with probability at least $1-\sum_{t=1}^{T}\tau_t^d\delta_t$, (\ref{pp:16}) is by hypothesis, as $\vx_t=\arg\max_{\vx\in\Xspace}(\wc{\mu_t}(\vx) + c^{+}\beta_t(\vx))$, and (\ref{aux:10}) is from (\ref{aux:7}) and holds with probability at least $1-\sum_{t=1}^{T}\delta_t$. Hence, (\ref{aux:10}) holds with probability no less than $1-\sum_{t=1}^{T}\tau_t^d\delta_t - \sum_{t=1}^{T}\delta_t = 1 - \sum_{t=1}^T\delta_t(1+\tau_t^d)$. Let us pick as a discretization schedule $\tau_t = dt^2$. This has no impact whatsoever on the algorithm, as the discretization is only hypothetical. With this $\tau_t$ and the confidence schedule proposed in the statement of the theorem, it is easy to verify that (\ref{aux:10}) holds with probability at least $1-\delta$.
	
	Finally, we can bound the regret. With probability at least $1-\delta$:
	\begin{align}
		\Reg(T) &\leq \Delta_0 + \sum_{t=1}^{T}\Delta_t \nonumber\\
		&\leq \Delta_0 + (c^{+}+c^{-})\sum_{t=1}^{T}\beta_t(x_t) + LDd\sum_{t=1}^{T}\frac{1}{\tau_t} \label{pp:17}\\
		&\leq (c^{+}+c^{-})\sum_{t=1}^{T}\beta_t(x_t) + \frac{\pi^2LD}{6} \label{pp:18}\\
		&\leq \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{\log\frac{1}{\delta_t}}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\pi^2LD}{6} \label{pp:19}\\
		&\leq \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{
			\log (1+d^dt^{2d}) + 2\log t + \log\frac{\pi^2}{6\delta}
		}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\pi^2LD}{6} \label{pp:20}\\
		&\leq \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{
		\log (2d^dt^{2d}) + 2\log t + \log\frac{\pi^2}{6\delta}
		}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\pi^2LD}{6} \label{pp:21}\\
		&= \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{
		2(d+1)\log t + d\log d + \log\frac{\pi^2}{3\delta}
		}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\pi^2LD}{6} \nonumber\\
		&\leq \Delta_0 + (c^{+}+c^{-})\left[v_{\epsilon}\left(
			2(d+1)\log T + d\log d + \log\frac{\pi^2}{3\delta}
		\right)\right]^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}
		t^{-\frac{\epsilon}{1+\epsilon}} + \frac{\pi^2LD}{6} 	\nonumber\\
		&\leq \Delta_0 + (c^{+}+c^{-})\left[v_{\epsilon}\left(
		2(d+1)\log T + d\log d + \log\frac{\pi^2}{3\delta}
		\right)\right]^{\frac{\epsilon}{1+\epsilon}}(1+\epsilon)T^{\frac{1}{1+\epsilon}}+ \frac{\pi^2LD}{6}, 	\label{pp:22}
	\end{align}
	where (\ref{pp:17}) is from (\ref{aux:10}) and holds with probability at least $1-\delta$, (\ref{pp:18}) is from the choice of $\tau_t$ and $\sum_{t=1}^{T}t^{-2}\leq\sum_{t=1}^{\infty}t^{-2} = \frac{\pi^2}{6}$, (\ref{pp:19}) is from Assumption \ref{ass:boundrenyi}, (\ref{pp:20}) is from the choice of $\delta_t$, (\ref{pp:21}) is from $\log(1+x)\leq\log(2x)$, which holds for every $x\geq 1$, and (\ref{pp:22}) is from  (\ref{aux:3}) with $\alpha=\frac{\epsilon}{1+\epsilon}$. 
	The proof is completed by renaming $C\gets (1+\epsilon)(c^{+}+c^{-}) = (1+\epsilon)(2\sqrt{2} + \frac{5}{3})\norm[\infty]{f}$.
\end{proof}

\regretdiscretized*
\begin{proof}
	The proof follows the one of Theorem \ref{th:regretcompact} up to (\ref{aux:9}), except from the fact that the discretization is actually performed by the algorithm. That is, with probability at least $1-\sum_{t=1}^{T}\delta_t(1+\tau_t^d)$:
	\begin{align}
		&\wc{\mu}_t(\vx_t) - \mu(\vx_t) \leq c^{-}\beta_t(\vx_t) &\text{and}\nonumber\\
		&\mu(\vx^*) - \wc{\mu}_t([\vx^*]_t) \leq c^{+}\beta_t([\vx^*]_t) + \frac{LDd}{\tau_t}
		&\text{for $t=1,\dots,T$ and every $\vx^*\in\arg\max_{\vx\in\Xspace}\mu(\vx)$.}\label{aux:11}
	\end{align}
	This is enough to bound the instantaneous regret. With probability at least $1-\sum_{t=1}^{T}\delta_t(1+\tau_t^d)$:
	\begin{align}
		\Delta_t = \mu(\vx^*) - \mu(\vx_t) 
		&\leq \wc{\mu}_t([\vx^*]_t) + c^{+}\beta_t([\vx^*]_t) + \frac{LDd}{\tau_t} - \mu(\vx_t) \label{pp:23}\\
		&\leq \wc{\mu}_t(\vx_t) + c^{+}\beta_t(\vx_t) + \frac{LDd}{\tau_t} - \mu(\vx_t) \label{pp:24}\\
		&\leq (c^{+}+c^{-})\beta_t(\vx_t) + \frac{LDd}{\tau_t}, \label{aux:12}
	\end{align}
	where (\ref{pp:22}) and (\ref{pp:24}) are from (\ref{aux:11}) and hold simultaneously with probability at least $1-\sum_{t=1}^{T}\delta_t(1+\tau_t^d)$, and (\ref{pp:23}) is by hypothesis, as $\vx_t = \arg\max_{\vx\in\wt{\Xspace}_t}(\wc{\mu}_t(\vx) + c^{+}\beta_t(\vx)$. Note that the latter is true only by virtue of the fact that both $[\vx^*]_t$ and $\vx_t$ belong to $\wt{\Xspace}_t$, as the optimization step of Algorithm \ref{alg:2} is restricted to~$\wt{\Xspace}_t$.
	
	Finally, we can bound the regret. With probability at least $1-\delta$:
	\begin{align}
		\Reg(T) &= \Delta_0 + \sum_{t=1}^{T}\Delta_t \nonumber\\
		&\leq \Delta_0 +(c^{+}+c^{-})\sum_{t=1}^{T}\beta_t(\vx_t) + LDd\sum_{t=1}^{T}\frac{1}{\tau_t} \label{pp:25}\\
		&\leq \Delta_0 +(c^{+}+c^{-})\sum_{t=1}^{T}\beta_t(\vx_t) + \frac{\kappa }{\kappa-1}LDT^{\left(1-\frac{1}{\kappa}\right)}d\label{pp:26}\\
		&\leq \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{\log\frac{1}{\delta_t}}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\kappa }{\kappa-1}LDT^{\left(1-\frac{1}{\kappa}\right)}d\label{pp:27}\\
		&\leq \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{
			2\log t + \log\left(1+\left\lceil t^{\frac{1}{\kappa}}\right\rceil^d\right) + \log\frac{\pi^2}{6\delta}
		}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\kappa }{\kappa-1}LDT^{\left(1-\frac{1}{\kappa}\right)}d\label{pp:28}\\
		&\leq \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{
		2\log t + d\log \left(t^{\frac{1}{\kappa}}+1\right) + \log\frac{\pi^2}{3\delta}
		}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\kappa }{\kappa-1}LDT^{\left(1-\frac{1}{\kappa}\right)}d\label{pp:29}\\
		&\leq \Delta_0 + (c^{+}+c^{-})v_{\epsilon}^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}\left(\frac{
		\left(2+ \frac{d}{\kappa}\right)\log t + d\log 2 + \log\frac{\pi^2}{3\delta}
		}{t}\right)^{\frac{\epsilon}{1+\epsilon}} + \frac{\kappa }{\kappa-1}LDT^{\left(1-\frac{1}{\kappa}\right)}d\label{pp:30}\\
		&\leq \Delta_0 + (c^{+}+c^{-})\left[v_{\epsilon}
		\left(
		\left(2+ \frac{d}{\kappa}\right)\log T + d\log 2 + \log\frac{\pi^2}{3\delta}
		\right)\right]^{\frac{\epsilon}{1+\epsilon}}\sum_{t=1}^{T}t^{-\frac{\epsilon}{1+\epsilon}} + \frac{\kappa }{\kappa-1}LDT^{\left(1-\frac{1}{\kappa}\right)}d, \nonumber\\
		&\leq \Delta_0 + (c^{+}+c^{-})\left[v_{\epsilon}
		\left(
		\left(2+ \frac{d}{\kappa}\right)\log T + d\log 2 + \log\frac{\pi^2}{3\delta}
		\right)\right]^{\frac{\epsilon}{1+\epsilon}}(1+\epsilon)T^{\frac{1}{1+\epsilon}} + \frac{\kappa }{\kappa-1}LDT^{\left(1-\frac{1}{\kappa}\right)}d, \nonumber\\
	\end{align}
	where (\ref{pp:25}) is from (\ref{aux:12}) and holds with probability at least $1-\delta$ with the proposed $\delta_t$ and $\tau_t$, (\ref{pp:26}) is from the proposed $\tau_t$ and (\ref{aux:3}) with $\alpha=\nicefrac{1}{\kappa}$,  (\ref{pp:27}) is from Assumption \ref{ass:boundrenyi}, (\ref{pp:28}) is from the proposed $\delta_t$, (\ref{pp:29}) and (\ref{pp:30}) are from the fact $\log(x+1)\leq\log(2x)$ for $x\geq 1$, and (\ref{pp:30}) is from (\ref{aux:3}) with $\alpha=\frac{\epsilon}{1+\epsilon}$. The proof is completed by renaming ${C_1\gets(1+\epsilon)(c^{+}+c^{-})\norm[\infty]{f} = (1+\epsilon)(2\sqrt{2}+\frac{5}{3})\norm[\infty]{f}}$ and $C_2\gets\frac{\kappa}{\kappa-1}LD$.
\end{proof}

%%
\begin{lemma}
	Let $\Psi = \sum_{l=1}^L \zeta_l P_l$ and $\Phi = \sum_{k=1}^K \beta_k Q_k$, with $\zeta_l \in [0,1]$, $\sum_{l=1}^L \zeta_l =1$, $\beta_k \in [0,1]$ and $\sum_{k=1}^K \beta_k =1$, be two finite mixtures of the probability measures $\{P_l\}_{l=1}^L$ and $\{Q_k\}_{k=1}^K$ respectively. Let $\{ \psi_{ij} \}_{\substack{i=1,2,...,L \\ j=1,2,...,K}}$ and $\{ \phi_{ij} \}_{\substack{i=1,2,...,L \\ j=1,2,...,K}}$ be two sets of variational parameters s.t. $\phi_{ij} \ge 0$, $\psi_{ij} \ge 0$, $\sum_{k=1}^K \phi_{ij}=\zeta_l$ and $\sum_{l=1}^L \psi_{ij}=\beta_k$. Then, for any $\alpha \ge 1$, it holds that: 
     \begin{equation*}
        d_{\alpha} (\Psi \| \Phi)^{\alpha-1} \le \sum_{l=1}^L \sum_{k=1}^K \phi_{lk}^\alpha \psi_{lk}^{1-\alpha} d_{\alpha} (P_l \| Q_k)^{\alpha-1}.
    \end{equation*}
\end{lemma}

\begin{proof}
Using the variational parameters we can express the two mixtures as:
\begin{align*}
    & \Psi = \sum_{l=1}^L \sum_{k=1}^K \phi_{lk} P_l,\\
    & \Phi = \sum_{l=1}^L \sum_{k=1}^K \psi_{lk} Q_k.
\end{align*}
    We use the convexity of the $d_{\alpha}$ and we apply Jensen inequality:
    \begin{align}
        d_{\alpha}(\Psi \| \Phi)^{\alpha-1} & = \int \left( \frac{\Psi}{\Phi} \right)^{\alpha} \mathrm{d}\Phi \notag\\
        & = \int \left( \sum_{l=1}^L \sum_{k=1}^K \frac{\phi_{lk} P_l}{\psi_{lk} Q_k} \frac{\psi_{lk} Q_k}{\Phi} \right)^{\alpha} \mathrm{d}\Phi \notag \\
        & \le \int \sum_{l=1}^L \sum_{k=1}^K \frac{\psi_{lk} Q_k}{\Phi} \left(  \frac{\phi_{lk} P_l}{\psi_{lk} Q_k}  \right)^{\alpha} \mathrm{d}\Phi \label{eq:strangeBound}\\
        & = \sum_{i=1}^n \sum_{j=1}^m \phi_{lk}^{\alpha} \psi_{lk}^{1-\alpha}  \int \left( \frac{P_l}{Q_k}  \right)^{\alpha} \mathrm{d}Q_k \notag \\
        & = \sum_{i=1}^n \sum_{j=1}^m \phi_{lk}^{\alpha} \psi_{lk}^{1-\alpha}  d_{\alpha}(P_l \| Q_k)^{\alpha-1}, \notag
    \end{align}
    where \eqref{eq:strangeBound} is obtained by Jensen inequality observing that $\frac{\psi_{lk} Q_k}{\Phi}$ is a distribution over $\{1,...,L\} \times \{1,...,K\}$.
\end{proof}

We now consider the case in which $f$ has just one mixture component, \ie $n = 1$. In this case, we have that $\sum_{i=1}^n \psi_{ij}= \psi_j = b_j$, therefore the result reduces to:
\begin{equation}
     d_{\alpha} (f \| g)^{\alpha-1} \le  \sum_{j=1}^m \phi_{j}^\alpha b_{j}^{1-\alpha} d_{\alpha} (f \| g_j)^{\alpha-1}.
\end{equation}
We can now minimize the bound over the $\phi_j$, subject to $\sum_{j=1}^m \phi_j = 1$, we get the following result.

\armonic*
\begin{proof}
We now consider the case in which $\Psi$ has just one mixture component, \ie $L = 1$ and we abbreviate $\Psi = P$. In this case, we have that $\sum_{l=1}^L \psi_{kl}= \psi_k = \beta_k$, therefore the result reduces to:
\begin{equation}
     d_{\alpha} (P \| \Phi)^{\alpha-1} \le  \sum_{k=1}^K \phi_{k}^\alpha \beta_{k}^{1-\alpha} d_{\alpha} (P \| Q_k)^{\alpha-1}.
\end{equation}
We can now minimize the bound over the $\phi_k$, subject to $\sum_{k=1}^K \phi_k = 1$. We use the Lagrange multipliers.
    \begin{equation*}
        \mathcal{L}(\{\phi_k\}_{k=1,2,...,K}, \lambda) = \sum_{k=1}^K \phi_{k}^\alpha \beta_{k}^{1-\alpha} d_{\alpha} (P \| Q_k)^{\alpha-1} - \lambda \left( \sum_{k=1}^K \phi_k -1 \right)
    \end{equation*}
    We take the partial derivatives \wrt the $\phi_k$ and the Lagrange multiplier $\lambda$.
    \begin{align*}
        \frac{\partial \mathcal{L}}{\partial \phi_k} = \alpha \phi_k^{\alpha-1} \beta_j^{1-\alpha} d_{\alpha}(P \| Q_k)^{\alpha-1} - \lambda = 0 \implies \phi_k = \frac{\lambda^{\frac{1}{\alpha-1}}  \beta_j}{\alpha^{\frac{1}{\alpha-1}} d_{\alpha}(P \| Q_k)}.
    \end{align*}
    We now replace the expression of $\phi_k$ into the constraint.
    \begin{align*}
        \sum_{j=1}^K \phi_k = \frac{\lambda^\frac{1}{\alpha-1}}{\alpha^\frac{1}{\alpha-1}}  \sum_{k=1}^L \frac{ \beta_k}{ d_{\alpha}(P \| Q_k)} = 1 \implies \lambda = \frac{\alpha}{\left( \sum_{k=1}^K \frac{ \beta_k}{ d_{\alpha}(P \| Q_k)}\right)^{{\alpha-1}}}.
    \end{align*}
    And finally we get the expression for $\phi_k$:
    \begin{equation}
        \phi_k = \frac{\frac{\beta_k}{d_{\alpha}(P \| Q_k)}} {\sum_{h=1}^K \frac{ \beta_h}{ d_{\alpha}(P \| Q_h)}}.
    \end{equation}
    We can now compute the bound value:
    \begin{align*}
        \sum_{k=1}^K \phi_{k}^\alpha \beta_{k}^{1-\alpha} d_{\alpha} (P \| Q_k)^{\alpha-1} & = \sum_{k=1}^K \frac{\frac{\beta_k^{\alpha}}{d_{\alpha}(P \| Q_k)^{\alpha}}} {\left(\sum_{h=1}^K \frac{ \beta_h}{ d_{\alpha}(P \| Q_h)}\right)^{\alpha}} \beta_{k}^{1-\alpha} d_{\alpha} (P \| Q_k)^{\alpha-1} \\
        & = \frac{ \sum_{k=1}^K \frac{\beta_k}{d_{\alpha}(P \| Q_k)} }{\left(\sum_{h=1}^K \frac{ \beta_h}{ d_{\alpha}(P \| Q_h)}\right)^{\alpha}} \\
        & = \frac{1} {\left(\sum_{k=1}^K \frac{ \beta_k}{ d_{\alpha}(P \| Q_k)}\right)^{\alpha-1}}.
    \end{align*}
    As a consequence the bound becomes:
    \begin{equation*}
        d_{\alpha}(P \| \Phi)^{\alpha-1} \le  \frac{1} {\left(\sum_{k=1}^K \frac{\beta_k}{ d_{\alpha}(P \| Q_k)}\right)^{\alpha-1}} \implies  d_{\alpha}(P \| \Phi) \le \frac{1} {\sum_{k=1}^K \frac{ \beta_k}{ d_{\alpha}(P \| Q_k)}},
    \end{equation*}
    which is the weighted harmonic mean of the exponentiated divergences.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}