\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

%Custom packages and macros
\usepackage{mymacros}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsthm,thmtools,thm-restate}
\usepackage{mathabx}
\usepackage{dsfont}
\usepackage{enumerate}

%Notes
\usepackage[colorinlistoftodos, textsize=tiny]{todonotes}
\definecolor{citrine}{rgb}{0.89, 0.82, 0.04}
\definecolor{blued}{RGB}{70,197,221}
%%todo by Marcello
\newcommand{\todomarc}[1]{\todo[color=orange, inline]{MARCELLO: #1}}
\newcommand{\todomarcout}[1]{\todo[color=orange]{M: #1}}
%%todo by Matteo
\newcommand{\todomat}[1]{\todo[color=green, inline]{MATTEO: #1}}
\newcommand{\todomatout}[1]{\todo[color=green]{M: #1}}
%%todo by Alberto
\newcommand{\todoalb}[1]{\todo[color=blued, inline]{ALBERTO: #1}}
\newcommand{\todoalbout}[1]{\todo[color=blued]{A: #1}}
%%todo by Lorenzo
\newcommand{\todolor}[1]{\todo[color=yellow, inline]{LORENZO: #1}}
\newcommand{\todolorout}[1]{\todo[color=yellow]{L: #1}}

%Theorems
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{sublemma}{Lemma}[section]
\newtheorem{assumption}{Assumption}

%Specific macros
\DeclareRobustCommand{\algoname}{TODO\@\xspace}

\allowdisplaybreaks[4]

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Short Title}

\begin{document}

\twocolumn[
\icmltitle{Exploration in Policy Search via Multiple Importance Sampling}

%TITLE IDEAS
	% Optimistic Policy Search/Optimization via Multiple Importance Sampling
	% Policy Exploration via Multiple Importance Sampling
	% Upper Confidence Policy Optimization/Search

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,poli}
\icmlauthor{Bauiu C.~Yyyy}{equal,poli}
\icmlauthor{Cieua Vvvvv}{poli}
\icmlauthor{Iaesut Saoeu}{poli}
\end{icmlauthorlist}

\icmlaffiliation{poli}{Politecnico di Milano, Milan, Italy}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\lipsum[1]
\end{abstract}

\section{Introduction}
\begin{itemize}
	\item Policy Search
	\item Exploration-exploitation tradeoff
	\item Bandits and OFU
	\item Exploration in RL and PS
	\item PS as a bandit problem
	\item Existing work on infinite arms
	\item Exploiting structure with MIS
	\item Heavy tails
	\item Contributions
	\item Structure
\end{itemize}

\section{Preliminaries}
In this section we provide an essential background on policy optimization and multiple importance sampling.
\subsection{Policy optimization}
In Policy Optimization~\citep{deisenroth2013survey} we seek the policy maximizing the agent's performance on a given RL task. The task is modeled as a continuous Markov Decision Process~\citep[MDP,][]{puterman2014markov} $\Task = \langle\Sspace,\Aspace,\Tran,\Rew,\gamma,\init\rangle$, where $\Sspace\in\Reals^{d_{\Sspace}}$ is the state space; $\Aspace\in\Reals^{d_{\Aspace}}$ is the action space; $\Tran:\Sspace\times\Aspace\to\Delta(\Sspace)$ is a Markovian transition kernel, such that, for each time $h$, the next state is drawn as $s_{h+1}\sim\Tran(\cdot|s_h,a_h)$ depending only on the current state and action; $\Rew:\Sspace\times\Aspace\to[-\Rmax,\Rmax]$ is a bounded reward signal, such that the next reward $r_{h+1} = \Rew(s_h,a_h)$ is a function of the current state and action, and $\Rmax>0$ is the maximum reward; $\gamma\in(0,1]$ is a discount factor; and $\init\in\Delta(\Sspace)$ is the initial state distribution, such that the initial state is drawn as $s_0\sim\mu$. The agent's behavior is modeled as a parametric policy $\pi_{\vtheta}:\Sspace\to\Delta(\Aspace)$, such that the current action is drawn as $a_h\sim\pi_{\vtheta}(\cdot|s_h)$ depending on the current state, where $\vtheta\in\Theta\subseteq\Reals^m$ are the policy parameters. Deterministic policies represent a special case where $\pi_{\vtheta}$ is a Dirac delta function. With abuse of notation, we write $a_h=\pi_{\vtheta}(s_h)$ in this case. We focus on the episodic setting, where the agent's experience is organized into finite trajectories of maximum length $H$, called the task's horizon\footnote{This is \wlg provided that the horizon $H$ is sufficiently long for the agent to reach steady optimal behavior}. A trajectory is a sequence of states and actions $\tau=[s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1}]$. Every policy $\pi_{\vtheta}$ induces a distribution over trajectories, whose density is denoted as $p_{\vtheta}$. Our basic measure of performance is the sum of discounted rewards over the trajectory:
\begin{align}\label{eq:Rtau}
	\Rew(\tau) = \sum_{h=0}^{H-1}\gamma^hr_{h+1}.
\end{align}
Let $J(\vtheta) = \Exp_{\tau\sim p_{\vtheta}}[\Rew(\tau)]$ be the expected performance under policy $\pi_{\vtheta}$. In an \textit{online learning} scenario, we aim to maximize the sum of expected performances over a sequence of episodes $t=0,\dots,T$. 
In the \textit{action-based} policy optimization paradigm~\citep{peters2008reinforcement}, the problem we want to solve is simply:
\begin{align}\label{eq:Jtheta}
	\max_{\vtheta_0,\dots,\vtheta_T\in\Theta} \Exp_{\tau_t\sim p_{\vtheta_t}}\left[\sum_{t=0}^T\Rew(\tau_t)\right] = 
	\max_{\vtheta_0,\dots,\vtheta_T\in\Theta} \sum_{t=0}^TJ(\vtheta_t),
\end{align}
where $\pi_{\vtheta_t}$ is the policy used for episode $t$.
In the action-based paradigm, stochastic policies are typically employed in order to ensure exploration, although deterministic policies have also been used with the addition of exogenous noise~\citep{silver2014deterministic}.
Instead, in the \textit{parameter-based} policy optimization paradigm~\citep{sehnke2008policy}, we define a distribution over policy parameters, $\nu_{\vxi}\in\Delta(\Theta)$, called \textit{hyperpolicy}, where $\vxi\in\Xi\subseteq\Reals^d$ are the hyperpolicy parameters, or \textit{hyperparameters}. For each episode $t$, policy parameters are drawn as $\vtheta_t\sim\nu_{\vxi_t}$ and the whole trajectory is executed with $\pi_{\vtheta_t}$. In this case, the optimization problem becomes:
\begin{align}\label{eq:Jx}
	\max_{\vxi_0,\dots,\vxi_T\in\Xi} \sum_{t=0}^T\Exp_{\vtheta_t\sim\nu_{\vxi_t}}\left[J(\vtheta_t)\right].
\end{align}
In the parameter-based paradigm, deterministic policies are typically employed, paired with stochastic hyperpolicies in order to ensure exploration.

\subsection{Multiple importance sampling}
Importance sampling~\citep{cochran2007sampling,mcbook} is a technique that allows to estimate the expectation of a function under some \textit{target} or \textit{proposal} distribution with samples drawn from a different distribution, called \textit{behavioral}. Multiple importance sampling~\citep{veach_optimally_1995} is a generalization of this technique that allows to employ, for the same estimation, samples drawn from several different behavioral distributions. Let $P$ and $Q$ be probability measures on a measurable space $(\mathcal{Z}, \mathcal{F})$, such that $P\ll Q$ (\ie $P$ is absolutely continuous \wrt $Q$). The importance weight $w_{P/Q}$ is the Radon-Nikodym derivative of $P$ \wrt $Q$, \ie~$w_{P/Q} \equiv \frac{\de P}{\de Q}$. Let $p$ and $q$ be the densities of $P$ and $Q$, respectively, \wrt a reference measure. From the chain rule, $w_{P/Q} = \frac{p}{q}$. In the continuous case, $p$ and $q$ are probability density functions (pdf's) of continuous random variables having laws $P$ and $Q$, respectively, and $w_{P/Q}$ is a likelihood ratio. 
Given a bounded function $f:\mathcal{Z}\to\Reals$, and a set of \iid outcomes $z_1,\dots,z_N$ sampled from $Q$, the importance sampling estimator of $\mu\coloneqq\Exp_{z\sim p}\left[f(z)\right]$ is:
\begin{align}\label{eq:ise}
	\wh{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N}f(z_i)w_{P/Q}(z_i),
\end{align}
which is an unbiased estimator, \ie ${\Exp_{z_i\simiid q}\left[\wh{\mu}_{IS}\right] = \mu}$. 

Now, let $Q_1,\dots,Q_K$ be all probability measures over the same probability space as $P$, and $P\ll Q_k$ for $k=1,\dots,K$. Let $\beta_1(z),\dots,\beta_K(z)$ be mixture weights, \ie for all $z\in\mathcal{Z}$, ${\beta_1(z)+\dots+\beta_K(z) = 1}$ and $\beta_k(z)\geq0$ for ${k=1,\dots,K}$. Let $z_{ik}$ denote the $i$-th sample drawn from $Q_k$. Given $N_k$ \iid samples from each $q_k$, the Multiple Importance Sampling estimator (MIS) is:
\begin{align}\label{eq:mise}
	\wh{\mu}_{\text{MIS}} = \sum_{k=1}^K\frac{1}{N_k}\sum_{i=1}^{N_k}\beta_k(z_{ik})w_{P/Q_k}(z_{ik})f(z_{ik}),
\end{align}
which is also an unbiased estimator of $\mu$ for any valid choice of the mixture weights. A common choice of the mixture weights having desirable variance properties is the balance heuristic~\citep{veach_optimally_1995}: 
\begin{align}\label{eq:bhw}
	\beta_k(z) = \frac{N_kq_k(z)}{\sum_{j=1}^{K}N_jq_j(z)},
\end{align}
which yields the Balance Heuristic estimator (BH):
\begin{align}\label{eq:bhe}
	\wh{\mu}_{\text{BH}} = \sum_{k=1}^K\sum_{i=1}^{N_k}\frac{p(z_{ik})}{\sum_{j=1}^K N_jq_j(z_{ik})}f(z_{ik}).
\end{align}
Since (\ref{eq:bhw}) are valid mixture weights, $\wh{\mu}_{\text{BH}}$ is an unbiased estimator of $\mu$. Moreover, its variance is not significantly larger than any other choice of the mixture weights~\citep[][Theorem 1]{veach_optimally_1995}.

To further characterize the variance of this estimator, we need the concept of \Renyi divergence. Given probability measures $P$ and $Q$ on $(\mathcal{Z},\mathcal{F})$, where $P\ll Q$ and $Q$ is $\sigma$-finite, the $\alpha$-\Renyi divergence is defined as:
\begin{align}\label{eq:renyi}
	D_{\alpha}(P\|Q) = \frac{1}{\alpha-1}\log\int_{\mathcal{Z}}\left(w_{P/Q}\right)^{\alpha}\de Q,
\end{align}
for $\alpha\in[0,\infty]$\footnote{The special cases $\alpha=0,1$ and $\infty$ are defined by taking limits.}.
We denote with $d_{\alpha}(P\|Q) = \exp\{D_{\alpha}(P\|Q)\}$ the exponentiated $\alpha$-\Renyi divergence. Of particular interest is $D_2$, as the variance of the importance weight is $\Var_{z\sim q}\left[w_{P/Q}(z)\right] = d_2(P\|Q) - 1$, which is a divergence itself~\citep{cortes2010learning}. For this reason, we always mean the $2$-\Renyi divergence when omitting the order $\alpha$. The \Renyi divergence was used by~\citet[][Lemma 4.1]{metelli2018policy} to upper bound the variance of the importance sampling estimator as $\Var_{z_i\simiid q}\left[\wh{\mu}_{\text{IS}}\right]\leq \norm[\infty]{f}^2d_2(P\|Q)/N$. A similar result can be derived for the BH estimator:
%
\begin{restatable}{lemma}{misevarbound}\label{lem:misevarbound}
	Let $P$ and $Q_k$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and $d_2(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals$ be a bounded function, \ie $\norm[\infty]{f}<\infty$. Let $\wh{\mu}_{\text{BH}}$ be the balance heuristic estimator of $f$, as defined in (\ref{eq:bhe}), using $N_k$ \iid samples from each $Q_k$. Then, the variance of $\wh{\mu}_{\text{BH}}$ can be upper bounded as:
	\begin{align*}
		\Var_{z_{ik}\simiid Q_k}\left[\wh{\mu}_{\text{BH}}\right] \leq \norm[\infty]{f}^2\frac{d_2(P\|\Phi)}{N},
	\end{align*}
	where ${N=\sum_{k=1}^{K}N_k}$ is the total number of samples and ${\Phi=\sum_{k=1}^K\frac{N_k}{N}Q_k}$ is a finite mixture.
\end{restatable}
%

\section{Robust Importance Weighted Estimation}\label{sec:robust}
In this section, we discuss how to perform a robust importance weighting estimation. It has been recently observed that, in many cases of interest, the plain estimator~\eqref{eq:ise} has problematic tail behavior~\cite{metelli2018policy}, preventing the use of exponential concentration inequalities.\footnote{Unless we require that $d_{\infty}(P \| \Phi)$ is finite, \ie that the importance weight have finite supremum, there always exists a value $\alpha>1$ such that $d_{\alpha}(P \| \Phi)=+\infty$.} A common heuristic to address this problem consists in truncating the weight to prevent it from assuming too large values~\cite{ionides2008truncated}:
\begin{align}\label{eq:truncatedise}
	\widecheck{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N} \min \left\{ M, {w}_{P/Q}(z_i) \right\} f(z_i),
\end{align}
where $\widecheck{w}_{P/Q}^M(z_i) = $ and $M_N$ is a threshold to limit the magnitude of the importance weight. Similarly, for the multiple importance sampling case, restricting to the BH, we have:
\begin{align}\label{eq:truncatedmise}
	\widecheck{\mu}_{\text{BH}} =\frac{1}{N} \sum_{k=1}^K\sum_{i=1}^{N_k} \min \left\{M,  \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} \right\} f(z_{ik}).
\end{align}
Clearly, since we are changing the importance weights we introduce a bias term, but reducing the range of the estimation we likely achieve a benefit in terms of variance. In the following, we present the bias-variance analysis of the estimator and we conclude by showing that we are able, using an adaptive truncation, to guarantee an exponential concentration.

\begin{restatable}{lemma}{truncatedbias}\label{lem:truncatedbias}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{\ge 0}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. Then, the bias of $\widecheck{\mu}_{\text{BH}}$ can be upper bounded as:
	 \begin{equation}
         0 \le \mu - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty} M^{-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon},
    \end{equation}
    and the variance of $\widecheck{\mu}_{\text{BH}}$ can be bounded as:
    \begin{equation}
    \label{eq:variancetruncated}
         \Var_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty}^2 M^{1-\epsilon} \frac{d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon}}{N},
    \end{equation}
	where ${N=\sum_{k=1}^{K}N_k}$ is the total number of samples and ${\Phi=\sum_{k=1}^K\frac{N_k}{N}Q_k}$ is a finite mixture.
\end{restatable}
%
It is worth noting that, when selecting $\epsilon=1$, equation~\eqref{eq:variancetruncated} reduces to Lemma~\ref{lem:misevarbound}, as the truncation operation can only reduce the variance. Clearly, the smaller we choose $M$ the larger would be the bias. Overall, we are interested in mining the joint contribution of bias and variance. Keeping $P$ and $\Phi$ fixed we observe that the bias depend on $M$ only, whereas the variance depends on $M$ and on the number of samples $N$. Intuitively, we may allow larger truncation thresholds $M$ as the number of samples $N$ increases. The following result states that, when using an \emph{adaptive threshold} depending on $N$, we are able to reach exponential concentration.
\begin{restatable}{theorem}{thrucatedconcentration}\label{lem:thrucatedconcentration}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{\ge 0}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. 
	Let $M_N = \left( \frac{N d_{1+\epsilon}\left( P \| \Phi  \right)^{\epsilon} }{\log \frac{1}{\delta}} \right) ^{\frac{1}{1+\epsilon}}$, then with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.1}
        \widecheck{\mu}_{\text{BH}} \le \mu + \|f\|_{\infty} \left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},
    \end{equation}
    and also, with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.2}
        \widecheck{\mu}_{\text{BH}} \ge \mu - \|f\|_{\infty} \left(\sqrt{2} + \frac{4}{3} \right) \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}}.
    \end{equation}
\end{restatable}

Our adaptive truncation approach and the consequent concentration results resemble the ones proposed in~\cite{bubeck2013bandits}. However, differently from~\cite{bubeck2013bandits} we do not remove sample with too high value, but we exploit the nature of the importance weighted estimator to just limit the weight magnitude. Indeed, this form of truncation turned out to be very effective in practice.


\section{Algorithm}
In this section, we use the mathematical tools presented so far to design a policy search algorithm that efficiently explores the space of solutions. The proposed algorithm, \algoname, is based on the Optimism in the Face of Uncertainty (OFU) principle and follows the Upper Confidence Bound (UCB) strategy~\citep{lai1985asymptotically,agrawal1995sample,auer2002finite} commonly used in Multi Armed Bandit (MAB) problems~\citep{robbins1985some,bubeck2012regret,lattimore2019bandit}. 

The problem we want to solve does \textit{not} fall into the MAB framework and requires its own formalization. Let $\Xspace\subseteq \Reals^d$ be our decision set, or \textit{arm set} in MAB jargon. Let $(\Omega,\mathcal{F},P)$ be a probability space. Let ${\{Z_{\vx}:\Omega\to\mathcal{Z}\mid \vx\in\Xspace\}}$ be a set of continuous random vectors parametrized by $\Xspace$, with common sample space ${\mathcal{Z}\subseteq \Reals^m}$. We denote with $p_{\vx}$ the probability density function of $Z_{\vx}$. Finally, let $f:\mathcal{Z}\to\Reals$ be a bounded \textit{payoff function}, and $\mu(\vx) = \Exp_{z\sim p_{\vx}}\left[f(z)\right]$ its expectation under $p_{\vx}$. For each iteration $t=0,\dots,T$, we select an arm $\vx_t$, draw a sample $z_t$ from $p_{\vx_t}$, and observe payoff $f(z_t)$, up to horizon $T$. The goal is to maximize the expected total payoff:
\begin{align}\label{eq:theproblem}
	\max_{\vx_0,\dots,\vx_T\in\Xspace} \Exp_{z_t\sim p_{\vx_t}}\left[\sum_{t=0}^T f(z_t)\right] = \max_{\vx_0,\dots,\vx_T\in\Xspace}\sum_{t=0}^{T}\mu(\vx_t).
\end{align}
Although we can evaluate $p_{\vx}$ for every $\vx\in\Xspace$, we can only observe $f(z_t)$ for the $z_t$ that are actually sampled. 
This models precisely the online, episodic policy optimization problem.
In action-based policy optimization, $\Xspace$ corresponds to the parameter space $\Theta$ of a class of stochastic policies ${\{\pi_{\vtheta}\mid\vtheta\in\Theta\}}$, $\mathcal{Z}$ to the set of possible trajectories, $p_{\vx}$ to the density $p_{\vtheta}$ over trajectories induced by policy $\pi_{\vtheta}$, and $f(z)$ to cumulated reward $\Rew(\tau)$. 
In parameter-based policy optimization, $\Xspace$ corresponds to the hyperparameter space $\Xi$ of a class of stochastic hyperpolicies $\{\nu_{\vxi}\mid\vxi\in\Xi\}$, $\mathcal{Z}$ to policy parameter space $\Theta$, $p_{\vx}$ to hyperpolicy $\nu_{\xi}$, and $f(z)$ to performance $J(\vtheta)$. In both cases, each iteration corresponds to a single episode, and horizon $T$ is the total number of episodes (not to be confused with the trajectory horizon $H$). From now on, we will refer to (\ref{eq:theproblem}) simply as the policy optimization problem.

To apply the UCB strategy to the policy optimization problem, we need an estimate of the objective $\mu(\vx)$ and a confidence region. 
For the former, we use the robust balance heuristic estimator $\wc{\mu}_{\text{BH}}$ from (\ref{eq:truncatedmise}). To simplify the notation, \wlg, we treat each sample $\vx$ as a distinct one. This corresponds to the case $K=t-1$ and $N_k\equiv1$. Hence, at each iteration $t$:
\begin{align}
	\wc{\mu}_t(\vx) = \sum_{k=0}^{t-1}
	\min\left\{M_{t-1}, \frac{p_{\vx}(z_k)}
	{\sum_{j=1}^{t-1}p_{\vx_j(z_k)}}\right\}f(z_k),
\end{align}
where ${M_{t} = \left(\frac{td_{1+\epsilon}(p_{\vx}\|\Phi_t)^{\epsilon}}{\log\frac{1}{\delta_t}}\right)^{\frac{1}{1+\epsilon}}}$ and ${\Phi_t = \frac{1}{t}\sum_{k=0}^{t-1}p_{\vx_k}}$.
According to Theorem \ref{lem:thrucatedconcentration}, the following \textit{index}:
\begin{align}
	&B_t(\vx,\delta_t) \coloneqq 
	\wc{\mu}_t(\vx) \nonumber\\
	&\quad+\norm[\infty]{f}\left(\sqrt{2}+\frac{4}{3}\right)
	\left(\frac{d_{1+\epsilon}(p_{\vx_t}\|\Phi_{t})\log\frac{1}{\delta_t}}{t}\right)^{\frac{\epsilon}{1+\epsilon}},
\end{align}
is an upper bound on $\mu(\vx)$ with probability at least $1-\delta_t$, \ie an upper confidence bound. The \algoname algorithm simply selects, at each iteration $t$, the arm with the largest value of the index $B_t(\vx)$. Pseudocode is provided in Algorithm \ref{alg:1}. The initial arm $\vx_0$ is arbitrary, as no prior information is available. The regret analysis of Section \ref{sec:regret} will provide a confidence schedule $(\delta_t)_{t=1}^T$. Knowledge of the actual horizon $T$ is not needed. Although any $\epsilon>0$ works, we suggest to use $\epsilon=1$ in practice, as it yields the more common $2$-\Renyi divergence. To be able to compute the indexes (or to perform any kind of index maximization), the algorithm needs to store all the $\vx_t$ together with the observed payoffs $f(z_t)$, hence $\mathcal{O}(Td)$ space is required, where $d$ is the dimensionality of the arm space $\Xspace$ (not to be confused with cardinality $|\Xspace|$, which may be infinite). The optimization step (line 4) may be very difficult when $\Xspace$ is not discrete~\citep[\cf][]{srinivas2009gaussian}. As often done in RL, we assume the time spent collecting samples dominates computational time. Recall that drawing a sample $z_t$ in Algorithm \ref{alg:1} corresponds to executing an entire trajectory of experience, which may be very time-consuming in real-world applications. Practical aspects of \algoname will be further discussed in Section \ref{sec:practical}.

\begin{algorithm}[t]
	\caption{\algoname}
	\label{alg:1}
	\begin{algorithmic}[1]
	\STATE {\bfseries Input:} initial arm $\vx_0$, confidence schedule $(\delta_t)_{t=1}^T$
	\STATE Draw sample $z_0\sim p_{\vx_0}$ and observe payoff $f(z_0)$
	\FOR{$t=1,\dots,T$}
		\STATE Select arm $\vx_t = \arg\max_{\vx\in\Xspace}B_t(\vx,\delta_t)$
		\STATE Draw sample $z_t\sim p_{\vx_t}$ and observe payoff $f(z_t)$
	\ENDFOR
	\end{algorithmic}
\end{algorithm}

\section{Regret Analysis}\label{sec:regret}
In this section, we provide high-probability guarantees on the quality of the solution provided by Algorithm \ref{alg:1}.
First, we rephrase the optimization problem (\ref{alg:1}) in terms of \textit{regret minimization}. The instantaneous regret is defined as:
\begin{align}\label{eq:inregret}
	\Delta_t = \mu(\vx^*) - \mu(\vx_t),
\end{align}
where $\vx^* = \arg\max_{\vx\in\Xspace}\mu(\vx)$. Let $\Reg(T) = \sum_{t=0}^T\Delta_t$ be the total regret.
As $\mu(\vx^*)$ is a constant, problem (\ref{eq:theproblem}) is trivially equivalent to:
\begin{align}\label{eq:regret}
	\min_{\vx_0,\dots,\vx_T\in\Xspace} \Reg(T).
\end{align}
In the following, we will show that Algorithm \ref{alg:1} yields sublinear regret under some mild assumptions. The proofs combine techniques from~\citet{srinivas2009gaussian} and~\citet{bubeck2013bandits} and are reported in Appendix \ref{app:proof}.
First, we need the following assumption on the \Renyi divergence:
%
\begin{restatable}{assumption}{boundrenyi}\label{ass:boundrenyi}
	For all $t=1,\dots,T$, the $(1+\epsilon)$-\Renyi divergence is uniformly bounded as:
	\begin{align*}
		\sup_{\vx_0,\vx_1,\dots,\vx_t\in\Xspace}d_{1+\epsilon}(p_{\vx_t}\|\Phi_t) = v_{\epsilon} < \infty,
	\end{align*}
	where $\Phi_t = \frac{1}{t}\sum_{k=0}^{t-1}p_{\vx_k}$,
\end{restatable}
%
which can be easily enforced through careful policy (or hyperpolicy) design. Next, we need some assumptions on the structure of the arm set $\Xspace$.

\subsection{Discrete arm set}
We start from the discrete case, where $|\mathcal{X}| = D \in \Naturals_{+}$.
This setting is particularly convenient, as the optimization step can be trivially solved in $\mathcal{O}(D)$ time. It is also of practical interest: even in applications where $\Xspace$ is naturally continuous (\eg robotics), the set of solutions that can be actually tried in practice may sometimes be constrained to a discrete, reasonably small set. In this simple setting, \algoname achieves $\wt{\mathcal{O}}(T^{\frac{1}{1+\epsilon}})$ regret:

\begin{restatable}{theorem}{regretdiscrete}\label{th:regretdiscrete}
	Let $\Xspace$ be a discrete arm set with ${|\mathcal{X}| = D \in \Naturals_{+}}$. Under Assumption \ref{ass:boundrenyi}, Algorithm \ref{alg:1} with confidence schedule $\delta_t = \frac{3\delta}{t^2\pi^2D}$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
		&\Reg(T) \leq \Delta_0 \\
		&\quad+ 	C
			T^{\frac{1}{1+\epsilon}}
			\left[v_{\epsilon}
			\left(2\log T + \log \frac{\pi^2D}{3\delta}\right)
			\right]^{\frac{\epsilon}{1+\epsilon}},
	\end{align*}
	where $C=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
This yields a $\wt{\mathcal{O}}(\sqrt{T})$ regret when $\epsilon=1$.

\subsection{Compact arm set}
Now, we consider the more general case of a compact arm set $\Xspace\in\Reals^d$. This case is also more interesting as it allows to tackle virtually any RL task. We can assume, \wlg, that $\Xspace$ is entirely contained in a box $[-D,D]^d$, with $D\in\Reals_{+}$. We also need the following assumption on the expected payoff:
%
\begin{restatable}{assumption}{lipschitz}\label{ass:lipschitz}
	The expected payoff $\mu$ is Lipschitz continuous, \ie, there exists a constant $L>0$ such that, for every $\vx, \vx'\in\Xspace$:
	\begin{align*}
		|\mu(\vx') - \mu(\vx)| \leq L\norm[1]{\vx - \vx'}.
	\end{align*}
\end{restatable}
%
This assumption is easily satisfied for policy optimization, as shown in the following:
%
\begin{restatable}{lemma}{lipschitzpol}\label{lem:lispschitzpol}
	In the policy optimization problem, Assumption \ref{ass:lipschitz} can be replaced by:
	\begin{align}\label{eq:lp1}
		\sup_{s\in\Sspace,\vtheta\in\Theta}\Exp_{a\sim\pi_{\vtheta}}
		\left[\left|\nabla_{\vtheta}\log\pi_{\vtheta}(a|s)\right|\right] = G_1 < \infty,
	\end{align}
	in the action-based paradigm, and by:
	\begin{align}\label{eq:lp2}
		\sup_{\vxi\in\Xi}\Exp_{\vtheta\sim\rho_{\vxi}}
		\left[\left|\nabla_{\vxi}\log\rho_{\vxi}(a|s)\right|\right] = G_2 < \infty,
	\end{align}
	in the parameter-based paradigm.
\end{restatable}
%
In the proof, we show how to derive the corresponding Lipschitz constants, and show how (\ref{eq:lp1}) and (\ref{eq:lp2}) are satisfied by the commonly-used Gaussian policy and hyperpolicy, respectively. This is enough for \algoname to achieve $\wt{\mathcal{O}}(d^{\frac{\epsilon}{1+\epsilon}}T^{\frac{1}{1+\epsilon}})$ regret:
%
\begin{restatable}{theorem}{regretcompact}\label{th:regretcompact}
	Let $\Xspace$ be a $d$-dimensional compact arm set with $\Xspace \subseteq [-D,D]^d$. Under Assumptions \ref{ass:boundrenyi} and \ref{ass:lipschitz}, Algorithm \ref{alg:1} with confidence schedule $\delta_t = \frac{6\delta}{\pi^2t^2(1+d^dt^{2d})}$ guarantees, with probability at least $1-\delta$:
	\begin{align*}
	&\Reg(T) \leq \Delta_0 \\
	&\quad+ 	C
	T^{\frac{1}{1+\epsilon}}
	\left[v_{\epsilon}
	\left(2(d+1)\log T + d\log d + \log \frac{\pi^2}{3\delta}\right)
	\right]^{\frac{\epsilon}{1+\epsilon}} \\
	&\quad+ \frac{\pi^2LD}{6},
	\end{align*}
	where $C=(1+\epsilon)\left(2\sqrt{2}+\frac{5}{3}\right)\norm[\infty]{f}$, and $\Delta_0$ is the instantaneous regret of the initial arm $\vx_0$.
\end{restatable}
%
This yields a $\wt{\mathcal{O}}(\sqrt{dT})$ regret when $\epsilon=1$.
\section{Practical Aspects}\label{sec:practical}
In this section, we discuss some practical aspects of \algoname. 

\subsection{Optimization}
\textcolor{blue}{
Some of the most relevant applications of policy search (\eg robotics) naturally involve infinite arm sets $\Xspace$. As mentioned before, optimization (line 4 in Algorithm \ref{alg:1}) can be very challenging in this scenario, as the index $B_t(\vx,\delta_t)$ is non-convex and non-differentiable. Global optimization methods could be applied at the cost of giving up theoretical guarantees. This direction may be beneficial in practice, but we leave it to future, more application-oriented work. Instead, we propose a general discretization method, inspired by the proof of Theorem \ref{th:regretcompact}, which preserves the regret bound for the compact case. The key intuition, common to several continuous MAB algorithms, is to make the discretization progressively finer. The pseudocode for this variant, called \algoname2, is reported in Algorithm \ref{alg:2}. Theorem \ref{th:regretcompact} applies to Algorithm \ref{alg:2} under the same hypotheses, so \algoname2 also achieves a $\wt{\mathcal{O}}(\sqrt{dT})$ regret. Unfortunately, this algorithm does not scale well with the dimensionality $d$ of the arm set, as the time per iteration is $O(d^dt^{2d})$.
%
\begin{algorithm}[t]
	\caption{\algoname2}
	\label{alg:2}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:} initial arm $\vx_0$, confidence schedule $(\delta_t)_{t=1}^T$
		\STATE Draw sample $z_0\sim p_{\vx_0}$ and observe payoff $f(z_0)$
		\FOR{$t=1,\dots,T$}
		\STATE Discretize $\Xspace$ with a uniform grid $\wt{\Xspace}_t$ of $(dt^2)^d$ points
		\STATE Select arm $\vx_t = \arg\max_{\vx\in\wt{\Xspace}_t}B_t(\vx,\delta_t)$
		\STATE Draw sample $z_t\sim p_{\vx_t}$ and observe payoff $f(z_t)$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
%
}

\subsection{Variational Bound for the Exponentiated Renyi Divergence between mixtures}
\todoalb{Adeguare notazione}
\algoname requires at each iteration to compute the exponentiated Renyi divergence between the currently considered distribution $p_{\vx}$ 
and the mixture $\Phi_t$, \ie $td_{1+\epsilon}(p_{\vx}\|\Phi_t)$. Even for standard cases, like Gaussian distributions,
this quantity cannot be obtained in closed form. In this section, we provide a variational upper bound for computing the exponentiated Renyi divergence
between mixtures, focusing on the case in which the first distribution is not a mixture. Consider two mixture distributions $f = \sum_{i=1}^n a_i f_i$ and $g = \sum_{j=1}^m b_j g_j$. We introduce two sets of variational parameters $\phi_{ij} \ge 0$ and $\psi_{ij} \ge 0$  such that $\sum_{j=1}^m \phi_{ij}=a_i$ and $\sum_{i=1}^n \psi_{ij}=b_j$. Using these parameters we can write:
\begin{align*}
    f = \sum_{i=1}^n \sum_{j=1}^m \phi_{ij} f_i, \qquad  g = \sum_{i=1}^n \sum_{j=1}^m \psi_{ij} g_j.
\end{align*}
We state the following preliminary result.
\begin{restatable}{lemma}{boundd2mixture}\label{lem:boundd2mixture}
    Let all involved symbols as defined before. Then, for any $\alpha \ge 1$ it holds that:
    \begin{equation*}
        d_{\alpha} (f \| g)^{\alpha-1} \le \sum_{i=1}^n \sum_{j=1}^m \phi_{ij}^\alpha \psi_{ij}^{1-\alpha} d_{\alpha} (f_i \| g_j)^{\alpha-1}.
    \end{equation*}
\end{restatable}
We now consider the case in which $f$ has just one mixture component, \ie $n = 1$. In this case, we have that $\sum_{i=1}^n \psi_{ij}= \psi_j = b_j$, therefore the result reduces to:
\begin{equation}
     d_{\alpha} (f \| g)^{\alpha-1} \le  \sum_{j=1}^m \phi_{j}^\alpha b_{j}^{1-\alpha} d_{\alpha} (f \| g_j)^{\alpha-1}.
\end{equation}
We can now minimize the bound over the $\phi_j$, subject to $\sum_{j=1}^m \phi_j = 1$, we get the following result.
\begin{restatable}{theorem}{armonic}\label{th:armonic}
    Let all involved symbols as defined before. Then, for any $\alpha \ge 1$ the bound is optimized by picking:
    \begin{equation}
        \phi_j = \frac{\frac{b_j}{d_{\alpha}(f \| g_j)}} {\sum_{k=1}^m \frac{ b_k}{ d_{\alpha}(f \| g_k)}},
    \end{equation}
    For this choice of $\phi_j$ we have:
    \begin{equation*}
        d_{\alpha}(f \| g) \le \frac{1} {\sum_{k=1}^m \frac{ b_k}{ d_{\alpha}(f \| g_k)}}
    \end{equation*}
\end{restatable}

\subsection{Parameter-based exploration}
\begin{itemize}
	\item Bound on $d_2$
\end{itemize}
\subsection{Action-based exploration}
\begin{itemize}
	\item Estimator of $d_2$
\end{itemize}

\section{Related Works}

\section{Experiments}

\section{Conclusion}

% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}
%
%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.
%
%If a paper is accepted, the final camera-ready version can (and
%probably should) include acknowledgements. In this case, please
%place such acknowledgements in an unnumbered section at the
%end of the paper. Typically, this will include thanks to reviewers
%who gave useful comments, to colleagues who contributed to the ideas,
%and to funding agencies and corporate sponsors that provided financial
%support.

\bibliography{../biblio}
\bibliographystyle{icml2019}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUPPLEMENTARY MATERIALS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\onecolumn
\appendix
\section{Proofs}\label{app:proof}

\misevarbound*
%
\begin{proof}
	The proof is similar to Lemma 4.1 of~\cite{metelli2018policy}:
    \begin{align}
    \Var_{z_{ik} \simiid Q_k} [\widehat{\mu}_{\text{BH}}]&  = \Var_{z_{ik} \simiid Q_k} \left[ \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{N_k}  f(z_{ki})\frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right] \\
    & = \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Var_{z_{ik} \sim Q_k} \left[   f(z_{ki}) \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right] \\
    & \le \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( f(z_{ki})   \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right)^2 \right]\\
    & \le \|f\|_{\infty} \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right)^2 \right]\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \left(\frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right)^2 \right]\\
    &= \|f\|_{\infty}^2 \frac{d_{2}(P\|\Phi)}{N},\\
    \end{align}
    \todoalb{Commentare passaggi}
\end{proof}


\truncatedbias*
\begin{proof}
Let us start with the bias term. The first inequality $0 \le \mu - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}]$ derives from the fact that $\widehat{\mu}_{\text{BH}} \ge \widecheck{\mu}_{\text{BH}}$, being $f(z) \ge 0$ for all $z$ and observing that $\widehat{\mu}$ is unbiased, \ie $\Exp_{z_{ik} \simiid Q_k}[\widehat{\mu}_{\text{BH}}]=\mu$. For the second inequality, let us consider the following inequalities:
    \begin{align}
    \mu - \Exp_{x_i \sim q_i} [\widecheck{\mu}]&  = \Exp_{z_{ik} \simiid Q_k}[\widehat{\mu}_{\text{BH}}] - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \\
    & =  \frac{1}{N} \sum_{k=1}^K\sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[  f(z_{ik}) \left( \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} - \min \left\{ M,  \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} \right\} \right) \right]\\    
    & =   \sum_{k=1}^K \frac{N_k}{N} \Exp_{z_{1k} \sim Q_k} \left[  f(z_{1k}) \left( \frac{p(z_{1k})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{1k})} - \min \left\{ M,  \frac{p(z_{1k})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{1k})} \right\} \right) \right]\\
     & = \Exp_{z \sim \Phi} \left[  f(z) \left( \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} - \min \left\{ M,  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right\} \right) \right]\\
    & =  \Exp_{z \sim \Phi} \left[  f(z) \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} - M  \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \Exp_{z \sim \Phi} \left[  f(z) \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon} \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{-\epsilon} \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon} \right] M^{-\epsilon}\\
        & = \|f\|_{\infty} d_{1+\epsilon}(P \| \Phi)^{\epsilon}  M^{-\epsilon},
    \end{align}
    \todoalb{Commentare passaggi}
    For the variance the argument is similar.
    \begin{align*}
    \Var_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}]&  = \Var_{z_{ik} \simiid Q_k} \left[ \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{N_k}  f(z_{ki}) \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right] \\
    & = \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Var_{z_{ik} \sim Q_k} \left[   f(z_{ki}) \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right] \\
    & \le \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( f(z_{ki})  \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right)^2 \right]\\
    & \le \|f\|_{\infty} \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right)^2 \right]\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^2 \right]\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^{1+\epsilon} \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^{1-\epsilon}  \right]\\
    & \le \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \left( \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon}  \right] M^{1-\epsilon}\\
    &= \|f\|_{\infty}^2 M^{1-\epsilon} \frac{d_{1+\epsilon}(P\|\Phi)^{\epsilon}}{N} ,\\
    \end{align*}
    \todoalb{Commentare passaggi}
\end{proof}

\thrucatedconcentration*

\begin{proof}
Let us start with the first inequality. Observing that all samples $z_{ik}$ are independent and that $\widecheck{\mu}_{\text{BH}} \le M \|f\|_{\infty}$, we can state using Bernstein inequality~\cite{boucheron2013concentration} that with probability at least $1-\delta$ we have:
    \begin{align}
         \widecheck{\mu}_{\text{BH}} & \le \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] + \sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} +\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N} \\
         & \le \mu + \|f\|_{\infty} \sqrt{\frac{2 M^{1-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon} \log  \frac{1}{\delta} }{N}} +\|f\|_{\infty} \frac{M \log \frac{1}{\delta}}{3N}\\
       & = \mu + \|f\|_{\infty}\left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},
    \end{align}
    For the second inequality we just need to consider the bias as well.
    \begin{align*}
         \widecheck{\mu}_{\text{BH}} & \ge  \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] - \sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} -\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N} \\
         & = \mu - \left(\mu - \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] \right) -\sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} -\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N}  \\
          & \ge \mu - \|f\|_{\infty} M^{-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon} - \|f\|_{\infty}\left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}}\\
          & = \mu - \|f\|_{\infty} \left(\sqrt{2} + \frac{4}{3} \right) \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},\\
    \end{align*}
    \todoalb{Commentare passaggi}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}