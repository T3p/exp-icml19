\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

%Custom packages and macros
\usepackage{mymacros}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsthm,thmtools,thm-restate}
\usepackage{mathabx}
\usepackage{dsfont}

%Notes
\usepackage[colorinlistoftodos, textsize=tiny]{todonotes}
\definecolor{citrine}{rgb}{0.89, 0.82, 0.04}
\definecolor{blued}{RGB}{70,197,221}
%%todo by Marcello
\newcommand{\todomarc}[1]{\todo[color=orange, inline]{MARCELLO: #1}}
\newcommand{\todomarcout}[1]{\todo[color=orange]{M: #1}}
%%todo by Matteo
\newcommand{\todomat}[1]{\todo[color=green, inline]{MATTEO: #1}}
\newcommand{\todomatout}[1]{\todo[color=green]{M: #1}}
%%todo by Alberto
\newcommand{\todoalb}[1]{\todo[color=blued, inline]{ALBERTO: #1}}
\newcommand{\todoalbout}[1]{\todo[color=blued]{A: #1}}
%%todo by Lorenzo
\newcommand{\todolor}[1]{\todo[color=yellow, inline]{LORENZO: #1}}
\newcommand{\todolorout}[1]{\todo[color=yellow]{L: #1}}

%Theorems
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{sublemma}{Lemma}[section]
\newtheorem{assumption}{Assumption}

%Specific macros
\DeclareRobustCommand{\algoname}{TODO\@\xspace}

\allowdisplaybreaks[4]

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Short Title}

\begin{document}

\twocolumn[
\icmltitle{Exploration in Policy Search via Multiple Importance Sampling}

%TITLE IDEAS
	% Optimistic Policy Search/Optimization via Multiple Importance Sampling
	% Policy Exploration via Multiple Importance Sampling
	% Upper Confidence Policy Optimization/Search

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,poli}
\icmlauthor{Bauiu C.~Yyyy}{equal,poli}
\icmlauthor{Cieua Vvvvv}{poli}
\icmlauthor{Iaesut Saoeu}{poli}
\end{icmlauthorlist}

\icmlaffiliation{poli}{Politecnico di Milano, Milan, Italy}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\lipsum[1]
\end{abstract}

\section{Introduction}
\begin{itemize}
	\item Policy Search
	\item Exploration-exploitation tradeoff
	\item Bandits and OFU
	\item Exploration in RL and PS
	\item PS as a bandit problem
	\item Existing work on infinite arms
	\item Exploiting structure with MIS
	\item Heavy tails
	\item Contributions
	\item Structure
\end{itemize}

\section{Preliminaries}
In this section we provide an essential background on policy optimization and multiple importance sampling.
\subsection{Policy optimization}
In Policy Optimization~\citep{deisenroth2013survey} we seek the policy maximizing the agent's performance on a given RL task. The task is modeled as a continuous Markov Decision Process~\citep[MDP,][]{puterman2014markov} $\Task = \langle\Sspace,\Aspace,\Tran,\Rew,\gamma,\init\rangle$, where $\Sspace\in\Reals^{d_{\Sspace}}$ is the state space; $\Aspace\in\Reals^{d_{\Aspace}}$ is the action space; $\Tran:\Sspace\to\Delta(\Aspace)$ is a Markovian transition kernel, such that, for each time $h$, the next state is drawn as $s_{h+1}\sim\Tran(\cdot|s_h,a_h)$ depending only on the current state and action; $\Rew:\Sspace\times\Aspace\to\Reals$ is a reward signal, such that the next reward $r_{h+1} = \Rew(s_h,a_h)$ is a function of the current state and action; $\gamma\in(0,1]$ is a discount factor; and $\init\in\Delta(\Sspace)$ is the initial state distribution, such that the initial state is drawn as $s_0\sim\mu$. The agent's behavior is modeled as a parametric policy $\pi_{\vtheta}:\Sspace\to\Delta(\Aspace)$, such that the current action is drawn as $a_t\sim\pi_{\vtheta}(\cdot|s_t)$ depending on the current state, where $\vtheta\in\Theta\subseteq\Reals^m$ are the policy parameters. Deterministic policies represent a special case where $\pi_{\vtheta}$ is a Dirac delta function. With abuse of notation, we write $a_h=\pi_{\vtheta}(s_h)$ in this case. We focus on the episodic setting, where the agent's experience is organized into finite trajectories of maximum length $H$, called the task's horizon\footnote{This is \wlg provided the horizon $H$ is sufficiently long for the agent to reach steady optimal behavior}. A trajectory is a sequence of states and action $\tau=[s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1}]$. Every policy $\pi_{\vtheta}$ induces a distribution over trajectories, whose density is denoted as $p_{\vtheta}$. We aim to maximize the sum of discounted rewards:
\begin{align}\label{eq:Rtau}
	\Rew(\tau) = \sum_{h=0}^{H-1}\gamma^hr_{h+1},
\end{align}
in expectation over all sources of randomness.
In \textit{action-based} policy optimization~\citep{peters2008reinforcement}, the problem is simply to find the performance-maximizing policy parameters:
\begin{align}\label{eq:Jtheta}
	\max_{\vtheta\in\Theta} J(\vtheta) \coloneqq \Exp_{\tau\sim p_{\vtheta}}\left[\Rew(\tau)\right].
\end{align}
In the action-based paradigm, stochastic policies are typically employed in order to ensure exploration, although deterministic policies have also been used with the addition of exogenous noise~\citep{silver2014deterministic}.
Instead, in \textit{parameter-based} policy optimization~\citep{sehnke2008policy}, we define a distribution over policy parameters, $\nu_{\vxi}\in\Delta(\Theta)$, called \textit{hyperpolicy}, where $\vxi\in\Xi\subseteq\Reals^d$ are the hyperpolicy parameters, or \textit{hyperparameters}. For each episode $t$, policy parameters are drawn as $\vtheta_t\sim\nu_{\vxi}$ and the whole trajectory is executed with $\pi_{\vtheta_t}$. In this case, we aim to find the performance-maximizing hyperparameters:
\begin{align}\label{eq:Jx}
	\max_{\vxi\in\Xi} \Exp_{\vtheta\sim\nu_{\vxi}}\left[J(\vtheta)\right],
\end{align}
where $J(\vtheta)$ is now a random variable.
In the parameter-based paradigm, deterministic policies are typically employed, paired with stochastic hyperpolicies in order to ensure exploration.

\subsection{Multiple importance sampling}
Importance sampling~\citep{cochran2007sampling,mcbook} is a technique that allows to estimate the expectation of a function under some \textit{target} or \textit{proposal} distribution with samples drawn from a different distribution, called \textit{behavioral}. Multiple importance sampling~\citep{veach_optimally_1995} is a generalization of this technique that allows to employ, for the same estimation, samples drawn from several different behavioral distributions. Let $P$ and $Q$ be probability measures on a measurable space $(\mathcal{Z}, \mathcal{F})$, such that $P\ll Q$ (\ie $P$ is absolutely continuous \wrt $Q$). The importance weight $w_{P/Q}$ is the Radon-Nikodym derivative of $P$ \wrt $Q$, \ie~$w_{P/Q} \equiv \frac{\de P}{\de Q}$. Let $p$ and $q$ be the densities of $P$ and $Q$, respectively, \wrt a reference measure. From the chain rule, $w_{P/Q} = \frac{p}{q}$. In the continuous case, $p$ and $q$ are probability density functions (pdf's) of continuous random variables having laws $P$ and $Q$, respectively, and $w_{P/Q}$ is a likelihood ratio. 
Given a bounded function $f:\mathcal{Z}\to\Reals$, and a set of \iid outcomes $z_1,\dots,z_N$ sampled from $Q$, the importance sampling estimator of $\mu\coloneqq\Exp_{z\sim p}\left[f(z)\right]$ is:
\begin{align}\label{eq:ise}
	\wh{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N}f(z_i)w_{P/Q}(z_i),
\end{align}
which is an unbiased estimator, \ie ${\Exp_{z_i\simiid q}\left[\wh{\mu}_{IS}\right] = \mu}$. 

Now, let $Q_1,\dots,Q_K$ be all probability measures over the same probability space as $P$, and $P\ll Q_k$ for $k=1,\dots,K$. Let $\beta_1(z),\dots,\beta_K(z)$ be mixture weights, \ie for all $z\in\mathcal{Z}$, ${\beta_1(z)+\dots+\beta_K(z) = 1}$ and $\beta_k(z)\geq0$ for ${k=1,\dots,K}$. Let $z_{ik}$ denote the $i$-th sample drawn from $Q_k$. Given $N_k$ \iid samples from each $q_k$, the Multiple Importance Sampling estimator (MIS) is:
\begin{align}\label{eq:mise}
	\wh{\mu}_{\text{MIS}} = \sum_{k=1}^K\frac{1}{N_k}\sum_{i=1}^{N_k}\beta_k(z_{ik})w_{P/Q_k}(z_{ik})f(z_{ik}),
\end{align}
which is also an unbiased estimator of $\mu$ for any valid choice of the mixture weights. A common choice of the mixture weights having desirable variance properties is the balance heuristic~\citep{veach_optimally_1995}: 
\begin{align}\label{eq:bhw}
	\beta_k(z) = \frac{N_kq_k(z)}{\sum_{j=1}^{K}N_jq_j(z)},
\end{align}
which yields the Balance Heuristic estimator (BH):
\begin{align}\label{eq:bhe}
	\wh{\mu}_{\text{BH}} = \sum_{k=1}^K\sum_{i=1}^{N_k}\frac{p(z_{ik})}{\sum_{j=1}^K N_jq_j(z_{ik})}f(z_{ik}).
\end{align}
Since (\ref{eq:bhw}) are valid mixture weights, $\wh{\mu}_{\text{BH}}$ is an unbiased estimator of $\mu$. Moreover, its variance is not significantly larger than any other choice of the mixture weights~\citep[][Theorem 1]{veach_optimally_1995}.

To further characterize the variance of this estimator, we need the concept of \Renyi divergence. Given probability measures $P$ and $Q$ on $(\mathcal{Z},\mathcal{F})$, where $P\ll Q$ and $Q$ is $\sigma$-finite, the $\alpha$-\Renyi divergence is defined as:
\begin{align}\label{eq:renyi}
	D_{\alpha}(P\|Q) = \frac{1}{\alpha-1}\log\int_{\mathcal{Z}}\left(w_{P/Q}\right)^{\alpha}\de Q,
\end{align}
for $\alpha\in[0,\infty]$\footnote{The special cases $\alpha=0,1$ and $\infty$ are defined by taking limits.}.
We denote with $d_{\alpha}(P\|Q) = \exp\{D_{\alpha}(P\|Q)\}$ the exponentiated $\alpha$-\Renyi divergence. Of particular interest is $D_2$, as the variance of the importance weight is $\Var_{z\sim q}\left[w_{P/Q}(z)\right] = d_2(P\|Q) - 1$, which is a divergence itself~\citep{cortes2010learning}. For this reason, we always mean the $2$-\Renyi divergence when omitting the order $\alpha$. The \Renyi divergence was used by~\citet[][Lemma 4.1]{metelli2018policy} to upper bound the variance of the importance sampling estimator as $\Var_{z_i\simiid q}\left[\wh{\mu}_{\text{IS}}\right]\leq \norm[\infty]{f}^2d_2(P\|Q)$. A similar result can be derived for the BH estimator:
%
\begin{restatable}{lemma}{misevarbound}\label{lem:misevarbound}
	Let $P$ and $Q_k$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and $d_2(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals$ be a bounded function, \ie $\norm[\infty]{f}<\infty$. Let $\wh{\mu}_{\text{BH}}$ be the balance heuristic estimator of $f$, as defined in (\ref{eq:bhe}), using $N_k$ \iid samples from each $Q_k$. Then, the variance of $\wh{\mu}_{\text{BH}}$ can be upper bounded as:
	\begin{align*}
		\Var_{z_{ik}\simiid Q_k}\left[\wh{\mu}_{\text{BH}}\right] \leq \norm[\infty]{f}^2\frac{d_2(P\|\Phi)}{N},
	\end{align*}
	where ${N=\sum_{k=1}^{K}N_k}$ is the total number of samples and ${\Phi=\sum_{k=1}^K\frac{N_k}{N}Q_k}$ is a finite mixture.
\end{restatable}
%

\subsection{Robust Importance Weighted Estimation}
In this section, we discuss how to perform a robust importance weighting estimation. It has been recently observed that, in many cases of interest, the plain estimator~\eqref{eq:ise} has problematic tail behavior~\cite{metelli_policy_2018}, preventing the use of exponential concentration inequalities.\footnote{Unless we require that $d_{\infty}(P \| \Phi)$ is finite, \ie that the importance weight have finite supremum, there always exists a value $\alpha>1$ such that $d_{\alpha}(P \| \Phi)=+\infty$.} A common heuristic to address this problem consists in truncate the weight to prevent it from assuming too large values~\cite{ionides2008truncated}:
\begin{align}\label{eq:truncatedise}
	\widecheck{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N} \min \left\{ M, {w}_{P/Q}(z_i) \right\} f(z_i),
\end{align}
where $\widecheck{w}_{P/Q}^M(z_i) = $ and $M_N$ is a threshold to limit the magnitude of the importance weight. Similarly, for the multiple importance sampling case, restricting to the BH, we have:
\begin{align}\label{eq:truncatedmise}
	\widecheck{\mu}_{\text{BH}} =\frac{1}{N} \sum_{k=1}^K\sum_{i=1}^{N_k} \min \left\{M,  \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} \right\} f(z_{ik}).
\end{align}
Clearly, since we are changing the importance weights we introduce a bias term, but reducing the range of the estimation we likely achieve a benefit in terms of variance. In the following, we will present the bias-variance analysis of the estimator and we conclude showing that we are able, using an adaptive truncation, to guarantee an exponential concentration.

\begin{restatable}{lemma}{truncatedbias}\label{lem:truncatedbias}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{\ge 0}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. Then, the bias of $\widecheck{\mu}_{\text{BH}}$ can be upper bounded as:
	 \begin{equation}
         0 \le \mu - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty} M^{-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon},
    \end{equation}
    and the variance of $\widecheck{\mu}_{\text{BH}}$ can be bounded as:
    \begin{equation}
    \label{eq:variancetruncated}
         \Var_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \le  \|f\|_{\infty}^2 M^{1-\epsilon} \frac{d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon}}{N},
    \end{equation}
	where ${N=\sum_{k=1}^{K}N_k}$ is the total number of samples and ${\Phi=\sum_{k=1}^K\frac{N_k}{N}Q_k}$ is a finite mixture.
\end{restatable}
%
It is worth noting that, when selecting $\epsilon=1$, equation~\eqref{eq:variancetruncated} reduces to Lemma~\ref{lem:misevarbound}, as the truncation operation can only reduce the variance. Clearly, the smaller we choose $M$ the larger would be the bias. Overall, we are interested in mining the joint contribution of bias and variance. Keeping $P$ and $\Phi$ fixed we observe that the bias depend on $M$ only, whereas the variance depends on $M$ and on the number of samples $N$. Intuitively, we may allow larger truncation thresholds $M$ as the number of samples $N$ increases. The following result states that, when using an \emph{adaptive threshold} depending on $N$, we are able to reach exponential concentration.
\begin{restatable}{theorem}{thrucatedconcentration}\label{lem:thrucatedconcentration}
	Let $P$ and $\{ Q_k \}_{k=1}^N$ be probability measures on the measurable space $(\mathcal{Z},\mathcal{F})$ such that $P\ll Q_k$ and there exists $\epsilon \in (0,1]$ s.t. $d_{1+\epsilon}(P\|Q_k)<\infty$ for $k=1,\dots,K$. Let $f:\mathcal{Z}\to\Reals_{\ge 0}$ be a bounded non-negative function, \ie $\norm[\infty]{f}<\infty$. Let $\widecheck{\mu}_{\text{BH}}$ be the truncated balance heuristic estimator of $f$, as defined in (\ref{eq:truncatedmise}), using $N_k$ \iid samples from each $Q_k$. 
	Let $M_N = \left( \frac{N d_{1+\epsilon}\left( P \| \Phi  \right)^{\epsilon} }{\log \frac{1}{\delta}} \right) ^{\frac{1}{1+\epsilon}}$, then with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.1}
        \widecheck{\mu}_{\text{BH}} \le \mu + \|f\|_{\infty} \left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},
    \end{equation}
    and also, with probability at least $1-\delta$:
    \begin{equation}\label{eq:1.2}
        \widecheck{\mu}_{\text{BH}} \ge \mu - \|f\|_{\infty} \left(\sqrt{2} + \frac{4}{3} \right) \left(\frac{ d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}}.
    \end{equation}
\end{restatable}

Our adaptive truncation approach and the consequent concentration results resemble the ones proposed in~\cite{bubeck2013bandits}. However, differently from~\cite{bubeck2013bandits} we do not remove sample with too high value, but we exploit the nature of the importance weighted estimator to just limit the weight magnitude. Indeed, this form of truncation turned out to be very effective in practice.


\section{Algorithm}
In this section, we use the mathematical tools presented so far to design a policy search algorithm that efficiently explores the space of solutions. The proposed algorithm, \algoname, is based on the Optimism in the Face of Uncertainty (OFU) principle and follows the Upper Confidence Bound (UCB) strategy~\citep{lai1985asymptotically,agrawal1995sample,auer2002finite} commonly used in Multi Armed Bandit (MAB) problems~\citep{robbins1985some,bubeck2012regret,lattimore2019bandit}. 

The problem we want to solve does \textit{not} fall into the MAB framework and requires its own formalization. Let $\Xspace\subseteq \Reals^d$ be our decision set, or \textit{arm set} in MAB jargon. Let $Z_{\vx}$ be a continuous random vector parametrized by $\vx\in\Xspace$, with sample space (range) ${\mathcal{Z}\subseteq \Reals^m}$. We denote with $p_{\vx}$ its probability density function. Finally, let $f:\mathcal{Z}\to\Reals$ be a bounded \textit{payoff function} and $\mu(\vx) = \Exp_{z\sim p_{\vx}}\left[f(z)\right]$. At each iteration $t$, we select an \textit{arm} $\vx_t$, draw a sample $z_t$ from $p_{\vx_t}$, and observe payoff $f(z_t)$, up to horizon $T$. The goal is to maximize the expected total payoff:
\begin{align}\label{eq:theproblem}
	\max_{\vx_1,\dots,\vx_T\in\Xspace} \Exp_{z_t\simiid p_{\vx_t}}\left[\sum_{t=1}^T f(z_t)\right] = \max_{\vx_1,\dots,\vx_T\in\Xspace}\sum_{t=1}^{T}\mu(\vx_t).
\end{align}
Although we can evaluate $p_{\vx}$ for every $\vx\in\Xspace$, we can only observe $f(z_t)$ for the $z_t$ that are actually sampled. 
This models precisely the online, episodic policy optimization problem.
In action-based policy optimization, $\Xspace$ corresponds to the parameter space $\Theta$ of a class of stochastic policies ${\{\pi_{\vtheta}\mid\vtheta\in\Theta\}}$, $\mathcal{Z}$ to the set of possible trajectories, $p_{\vx}$ to the density $p_{\vtheta}$ over trajectories induced by policy $\pi_{\vtheta}$, and $f(z)$ to cumulated reward $\Rew(\tau)$. 
In parameter-based policy optimization, $\Xspace$ corresponds to the hyperparameter space $\Xi$ of a class of stochastic hyperpolicies $\{\nu_{\vxi}\mid\vxi\in\Xi\}$, $\mathcal{Z}$ to policy parameter space $\Theta$, $p_{\vx}$ to hyperpolicy $\nu_{\xi}$, and $f(z)$ to performance $J(\vtheta)$. In both cases, each iteration corresponds to a single episode, and horizon $T$ is the total number of episodes (not to be confused with the trajectory horizon $H$).

To apply the UCB strategy to our our problem (\ref{eq:theproblem}), we need an estimate of the objective $\mu(\vx)$ and a confidence region. 
For the former, we use the robust balance heuristic estimator $\wc{\mu}_{\text{BH}}$ from (\ref{eq:truncatedmise}). To simplify the notation, \wlg, we treat each sample $\vx$ as a distinct one. This corresponds to the case $K=t-1$ and $N_k\equiv1$. Hence, at each iteration $t$:
\begin{align}
	\wc{\mu}_t(\vx) = \sum_{k=1}^{t-1}
	\min\left\{M_{t-1}, \frac{p_{\vx}(z_k)}
	{\sum_{j=1}^{t-1}p_{\vx_j(z_k)}}\right\}f(z_k),
\end{align}
where ${M_{t-1} = \left(\frac{(t-1)d_{1+\epsilon}(p_{\vx}\|\Phi_t)}{\log\frac{1}{\delta_t}}\right)^{\frac{1}{1+\epsilon}}}$ and ${\Phi_t = \frac{1}{t-1}\sum_{k=1^{t-1}}p_{\vx_k}}$.
According to Theorem \ref{lem:thrucatedconcentration}, the following \textit{index}:
\begin{align}
	B_t(\vx) &\coloneqq 
	\wc{\mu}_t(\vx) \nonumber\\
	&+\norm[\infty]{f}\left(\sqrt{2}+\frac{4}{3}\right)
	\left(\frac{d_{1+\epsilon}(p_{\vx_t}\|\Phi_{t})\log\frac{1}{\delta_t}}{t-1}\right)^{\frac{\epsilon}{1+\epsilon}},
\end{align}
is an upper bound on $\mu(\vx_t)$ with probability at least $1-\delta_t$, \ie an upper confidence bound. The \algoname algorithm simply selects, at each iteration $t$, the arm with the largest value of the index $B_t(\vx)$. Pseudocode is provided in Algorithm TODOREF. The choice of confidence schedule $(\delta_t)_t$ is justified by the regret analysis of Section \ref{sec:regret}. Although any $\epsilon>0$ works, we suggest to use $\epsilon=1$ in practice, as it yields the more common $2$-\Renyi divergence. To be able to compute the indexes (or to perform any kind of index maximization), the algorithm needs to store all the $\vx_t$ together with the observed payoffs $f(\vx_t)$. Optimization may be very difficult when $\Xspace$ is not discrete. As always in RL, we assume the cost of collecting samples dominates computational costs. Practical aspects of \algoname will be further discussed in Section \ref{sec:practical}

\section{Regret Analysis}\label{sec:regret}
\begin{itemize}
	\item Finite arms
	\item Compact arm set
\end{itemize}

\section{Practical Aspects}\label{sec:practical}
\begin{itemize}
	\item The optimization problem
\end{itemize}
\subsection{Parameter-based exploration}
\begin{itemize}
	\item Bound on $d_2$
\end{itemize}
\subsection{Action-based exploration}
\begin{itemize}
	\item Estimator of $d_2$
\end{itemize}

\section{Related Works}

\section{Experiments}

\section{Conclusion}

% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}
%
%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.
%
%If a paper is accepted, the final camera-ready version can (and
%probably should) include acknowledgements. In this case, please
%place such acknowledgements in an unnumbered section at the
%end of the paper. Typically, this will include thanks to reviewers
%who gave useful comments, to colleagues who contributed to the ideas,
%and to funding agencies and corporate sponsors that provided financial
%support.

\bibliography{../biblio}
\bibliographystyle{icml2019}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUPPLEMENTARY MATERIALS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\onecolumn
\appendix
\section{Proofs}

\misevarbound*
%
\begin{proof}
	The proof is similar to Lemma 4.1 of~\cite{metelli2018policy}:
    \begin{align}
    \Var_{z_{ik} \simiid Q_k} [\widehat{\mu}_{\text{BH}}]&  = \Var_{z_{ik} \simiid Q_k} \left[ \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{N_k}  f(z_{ki})\frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right] \\
    & = \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Var_{z_{ik} \sim Q_k} \left[   f(z_{ki}) \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right] \\
    & \le \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( f(z_{ki})   \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right)^2 \right]\\
    & \le \|f\|_{\infty} \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right)^2 \right]\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \left(\frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right)^2 \right]\\
    &= \|f\|_{\infty}^2 \frac{d_{2}(P\|\Phi)}{N},\\
    \end{align}
    \todoalb{Commentare passaggi}
\end{proof}


\truncatedbias*
\begin{proof}
Let us start with the bias term. The first inequality $0 \le \mu - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}]$ derives from the fact that $\widehat{\mu}_{\text{BH}} \ge \widecheck{\mu}_{\text{BH}}$, being $f(z) \ge 0$ for all $z$ and observing that $\widehat{\mu}$ is unbiased, \ie $\Exp_{z_{ik} \simiid Q_k}[\widehat{\mu}_{\text{BH}}]=\mu$. For the second inequality, let us consider the following inequalities:
    \begin{align}
    \mu - \Exp_{x_i \sim q_i} [\widecheck{\mu}]&  = \Exp_{z_{ik} \simiid Q_k}[\widehat{\mu}_{\text{BH}}] - \Exp_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}] \\
    & =  \frac{1}{N} \sum_{k=1}^K\sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[  f(z_{ik}) \left( \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} - \min \left\{ M,  \frac{p(z_{ik})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{ik})} \right\} \right) \right]\\    
    & =   \sum_{k=1}^K \frac{N_k}{N} \Exp_{z_{1k} \sim Q_k} \left[  f(z_{1k}) \left( \frac{p(z_{1k})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{1k})} - \min \left\{ M,  \frac{p(z_{1k})}{\sum_{j=1}^K \frac{N_j}{N} q_j(z_{1k})} \right\} \right) \right]\\
     & = \Exp_{z \sim \Phi} \left[  f(z) \left( \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} - \min \left\{ M,  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right\} \right) \right]\\
    & =  \Exp_{z \sim \Phi} \left[  f(z) \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} - M  \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \Exp_{z \sim \Phi} \left[  f(z) \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right) \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon} \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{-\epsilon} \mathds{1}_{\left\{ \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \ge M \right\} } \right]\\
    & \le \|f\|_{\infty} \Exp_{z \sim \Phi} \left[  \left(  \frac{p(z)}{\sum_{j=1}^K \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon} \right] M^{-\epsilon}\\
        & = \|f\|_{\infty} d_{1+\epsilon}(P \| \Phi)^{\epsilon}  M^{-\epsilon},
    \end{align}
    \todoalb{Commentare passaggi}
    For the variance the argument is similar.
    \begin{align*}
    \Var_{z_{ik} \simiid Q_k} [\widecheck{\mu}_{\text{BH}}]&  = \Var_{z_{ik} \simiid Q_k} \left[ \frac{1}{N} \sum_{k=1}^K \sum_{i=1}^{N_k}  f(z_{ki}) \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right] \\
    & = \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Var_{z_{ik} \sim Q_k} \left[   f(z_{ki}) \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right] \\
    & \le \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( f(z_{ki})  \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right)^2 \right]\\
    & \le \|f\|_{\infty} \frac{1}{N^2} \sum_{k=1}^K \sum_{i=1}^{N_k} \Exp_{z_{ik} \sim Q_k} \left[   \left( \min \left\{ M, \frac{p(z_{ki})}{\sum_{j=1}^n \frac{N_j}{N} q_j(z_{ki})} \right\} \right)^2 \right]\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^2 \right]\\
    & = \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^{1+\epsilon} \min \left\{ M, \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right\}^{1-\epsilon}  \right]\\
    & \le \|f\|_{\infty}^2 \frac{1}{N} \Exp_{z \sim \Phi} \left[ \left( \frac{p(z)}{\sum_{j=1}^n \frac{N_j}{N} q_j(z)} \right)^{1+\epsilon}  \right] M^{1-\epsilon}\\
    &= \|f\|_{\infty}^2 M^{1-\epsilon} \frac{d_{1+\epsilon}(P\|\Phi)^{\epsilon}}{N} ,\\
    \end{align*}
    \todoalb{Commentare passaggi}
\end{proof}

\thrucatedconcentration*

\begin{proof}
Let us start with the first inequality. Observing that all samples $z_{ik}$ are independent and that $\widecheck{\mu}_{\text{BH}} \le M \|f\|_{\infty}$, we can state using Bernstein inequality~\cite{} that with probability at least $1-\delta$ we have:
    \begin{align}
         \widecheck{\mu}_{\text{BH}} & \le \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] + \sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} +\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N} \\
         & \le \mu + \|f\|_{\infty} \sqrt{\frac{2 M^{1-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon} \log  \frac{1}{\delta} }{N}} +\|f\|_{\infty} \frac{M \log \frac{1}{\delta}}{3N}\\
       & = \mu + \|f\|_{\infty}\left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},
    \end{align}
    For the second inequality we just need to consider the bias as well.
    \begin{align*}
         \widecheck{\mu}_{\text{BH}} & \ge  \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] - \sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} -\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N} \\
         & = \mu - \left(\mu - \Exp_{z_{ik} \sim Q_k} [\widecheck{\mu}_{\text{BH}}] \right) -\sqrt{2 \Var_{z_{ik} \simiid Q_k}[\widecheck{\mu}_{\text{BH}}] \log \frac{1}{\delta}} -\|f\|_{\infty}  \frac{M  \log \frac{1}{\delta}}{3N}  \\
          & \ge \mu - \|f\|_{\infty} M^{-\epsilon} d_{1+\epsilon}\left( P \| \Phi \right)^{\epsilon} - \|f\|_{\infty}\left(\sqrt{2} + \frac{1}{3} \right)  \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}}\\
          & = \mu - \|f\|_{\infty} \left(\sqrt{2} + \frac{4}{3} \right) \left(\frac{d_{1+\epsilon}\left( P \| \Phi  \right) \log  \frac{1}{\delta}  }{N} \right)^{\frac{\epsilon}{1+\epsilon}},\\
    \end{align*}
    \todoalb{Commentare passaggi}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}