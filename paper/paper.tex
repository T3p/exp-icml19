\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

%Custom packages and macros
\usepackage{mymacros}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{mathtools}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Short Title}

\begin{document}

\twocolumn[
\icmltitle{Exploration in Policy Search via Multiple Importance Sampling}

%TITLE IDEAS
	% Optimistic Policy Search/Optimization via Multiple Importance Sampling
	% Policy Exploration via Multiple Importance Sampling
	% Upper Confidence Policy Optimization/Search

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,poli}
\icmlauthor{Bauiu C.~Yyyy}{equal,poli}
\icmlauthor{Cieua Vvvvv}{poli}
\icmlauthor{Iaesut Saoeu}{poli}
\end{icmlauthorlist}

\icmlaffiliation{poli}{Politecnico di Milano, Milan, Italy}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\lipsum[1]
\end{abstract}

\section{Introduction}
\begin{itemize}
	\item Policy Search
	\item Exploration-exploitation tradeoff
	\item Bandits and OFU
	\item Exploration in RL and PS
	\item PS as a bandit problem
	\item Existing work on infinite arms
	\item Exploiting structure with MIS
	\item Heavy tails
	\item Contributions
	\item Structure
\end{itemize}

\section{Preliminaries}
In this section we provide an essential background on policy optimization and multiple importance sampling.
\subsection{Policy optimization}
In Policy Optimization~\citep{deisenroth2013survey} we seek the policy maximizing the agent's performance on a given RL task. The task is modeled as a continuous Markov Decision Process~\citep[MDP,][]{puterman2014markov} $\Task = \langle\Sspace,\Aspace,\Tran,\Rew,\gamma,\init\rangle$, where $\Sspace\in\Reals^{d_{\Sspace}}$ is the state space; $\Aspace\in\Reals^{d_{\Aspace}}$ is the action space; $\Tran:\Sspace\to\Delta(\Aspace)$ is a Markovian transition kernel, such that, for each time $h$, the next state is drawn as $s_{h+1}\sim\Tran(\cdot|s_h,a_h)$ depending only on the current state and action; $\Rew:\Sspace\times\Aspace\to\Reals$ is a reward signal, such that the next reward $r_{h+1} = \Rew(s_h,a_h)$ is a function of the current state and action; $\gamma\in(0,1]$ is a discount factor; and $\init\in\Delta(\Sspace)$ is the initial state distribution, such that the initial state is drawn as $s_0\sim\mu$. The agent's behavior is modeled as a parametric policy $\pi_{\vtheta}:\Sspace\to\Delta(\Aspace)$, such that the current action is drawn as $a_t\sim\pi_{\vtheta}(\cdot|s_t)$ depending on the current state, where $\vtheta\in\Theta\subseteq\Reals^m$ are the policy parameters. Deterministic policies represent a special case where $\pi_{\vtheta}$ is a Dirac delta function. With abuse of notation, we write $a_h=\pi_{\vtheta}(s_h)$ in this case. We focus on the episodic setting, where the agent's experience is organized into finite trajectories of maximum length $H$, called the task's horizon\footnote{This is \wlg provided the horizon $H$ is sufficiently long for the agent to reach steady optimal behavior}. A trajectory is a sequence of states and action $\tau=[s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1}]$. Every policy $\pi_{\vtheta}$ induces a distribution over trajectories, denoted as $p_{\vtheta}$. We aim to maximize the sum of discounted rewards:
\begin{align}\label{eq:Rtau}
	\Rew(\tau) = \sum_{h=0}^{H-1}\gamma^hr_{h+1},
\end{align}
in expectation over all sources of randomness.
In \textit{action-based} policy optimization~\citep{peters2008reinforcement}, the problem is simply to find the performance-maximizing policy parameters:
\begin{align}\label{eq:Jtheta}
	\max_{\vtheta\in\Theta} J(\vtheta) \coloneqq \Exp_{\tau\sim p_{\vtheta}}\left[\Rew(\tau)\right].
\end{align}
In the action-based paradigm, stochastic policies are typically employed in order to ensure exploration, although deterministic policies have also been used with the addition of exogenous noise~\citep{silver2014deterministic}.
Instead, in \textit{parameter-based} policy optimization~\citep{sehnke2008policy}, we define a distribution over policy parameters, $\nu_{\vx}\in\Delta(\Theta)$, called \textit{hyperpolicy}, where $\vx\in\Xspace\subseteq\Reals^d$ are the hyperpolicy parameters, or \textit{hyperparameters}. For each episode $t$, policy parameters are drawn as $\vtheta_t\sim\nu_{\vx}$ and the whole trajectory is executed with $\pi_{\vtheta_t}$. In this case, we aim to find the performance-maximizing hyperparameters:
\begin{align}\label{eq:Jx}
	\max_{\vx\in\Xspace} \Exp_{\vtheta\sim\nu_{\vx}}\left[J(\vtheta)\right],
\end{align}
where $J(\vtheta)$ is now a random variable.
In the parameter-based paradigm, deterministic policies are typically employed, paired with stochastic hyperpolicies in order to ensure exploration.

\subsection{Multiple importance sampling}
Importance sampling~\citep{cochran2007sampling,mcbook} is a technique that allows to estimate the expectation of a function under some \textit{target} or \textit{proposal} distribution with samples drawn from a different distribution, called \textit{behavioral}. Multiple importance sampling~\citep{veach_optimally_1995} is a generalization of this technique that allows to employ, for the same estimation, samples drawn from several different behavioral distributions. Let $P$ and $Q$ be probability measures over some $\sigma$-algebra of sample space $\mathcal{Z}$, such that $P\ll Q$ (\ie $P$ is absolutely continuous \wrt $Q$). The importance weight $w_{P/Q}$ is the Radon-Nikodym derivative of $P$ \wrt $Q$, \ie~$w_{P/Q} \equiv \frac{dP}{dQ}$. We focus on the continuous case, where $P$ and $Q$ are the laws of continuous random variables having probability density functions $p$ and $q$, respectively. In this case, the importance weight is equivalent to the likelihood ratio, \ie $w_{P/Q} = \frac{p}{q}$. 
Given a bounded function $f:\mathcal{Z}\to\Reals$, and a set of samples $z_1,\dots,z_N$ drawn from $Q$, the importance sampling estimator of $\Exp_{z\sim p}\left[f(z)\right]$ is:
\begin{align}\label{eq:ise}
	\wh{\mu}_{\text{IS}} = \frac{1}{N}\sum_{i=1}^{N}f(z_i)w_{P/Q}(z_i),
\end{align}
which is an unbiased estimator, \ie ${\Exp_{z_i\sim q}\left[\wh{\mu}_{IS}\right] = \Exp_{z\sim p}[f(z)]}$. Now, let $Q_1,\dots,Q_K$ be all probability measures over the same probability space as $P$, and $P\ll Q_i$ for $i=1,\dots,K$. Let $\alpha_1(z),\dots,\alpha_K(z)$ be mixture weights, \ie for all $z\in\mathcal{Z}$, ${\alpha_1(z)+\dots+\alpha_K(z) = 1}$ and $\alpha_k(z)\geq0$ for ${k=1,\dots,K}$. Let $z_{ik}$ be the $i$-th sample drawn from $Q_k$. Given $N_k$ samples from each $Q_k$, the Multiple Importance Sampling estimator (MIS) is:
\begin{align}\label{eq:mise}
	\wh{\mu}_{\text{MIS}} = \sum_{k=1}^K\frac{1}{N_k}\sum_{i=1}^{N_k}\alpha_k w_{P/Q_k}(z_{ik})f(z_{ik}),
\end{align}
which is also an unbiased estimator of $\Exp_{z\sim p}\left[f(z)\right]$ for any choice of the mixture weights. A common choice of the mixture weights having desirable variance properties is the balance heuristic~\citep{veach_optimally_1995}: 
\begin{align}
	a
\end{align}

\section{Algorithm}
\begin{itemize}
	\item OFU principle
	\item Robust estimator
	\item Pseudocode
\end{itemize}

\section{Regret Analysis}
\begin{itemize}
	\item Finite arms
	\item Compact arm set
\end{itemize}

\section{Practical Aspects}
\begin{itemize}
	\item The optimization problem
\end{itemize}
\subsection{Parameter-based exploration}
\begin{itemize}
	\item Bound on $d_2$
\end{itemize}
\subsection{Action-based exploration}
\begin{itemize}
	\item Estimator of $d_2$
\end{itemize}

\section{Related Works}

\section{Experiments}

\section{Conclusion}

% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}
%
%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.
%
%If a paper is accepted, the final camera-ready version can (and
%probably should) include acknowledgements. In this case, please
%place such acknowledgements in an unnumbered section at the
%end of the paper. Typically, this will include thanks to reviewers
%who gave useful comments, to colleagues who contributed to the ideas,
%and to funding agencies and corporate sponsors that provided financial
%support.

\bibliography{../biblio}
\bibliographystyle{icml2019}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUPPLEMENTARY MATERIALS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{Proofs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}